{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c30b81bdea7445f8eb66f47246d7fd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9acbfc9f4b84a55811c1480684a7df9",
              "IPY_MODEL_a1193cc0e7bd4ed1bca2014d27c2c210",
              "IPY_MODEL_13cacae19c234486be9e1583c7400e2d"
            ],
            "layout": "IPY_MODEL_fc4a47706081462f9ee42dd68cce2b1a"
          }
        },
        "f9acbfc9f4b84a55811c1480684a7df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_936c68da4e6644c8bc3e899a21867f25",
            "placeholder": "​",
            "style": "IPY_MODEL_a04791b981bd40aab11e3d96930c97b6",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "a1193cc0e7bd4ed1bca2014d27c2c210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb4bf18f7129459ea7b60319b33e4e64",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a6e4085be214aa0b973b0931a535c55",
            "value": 200
          }
        },
        "13cacae19c234486be9e1583c7400e2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88b56ed1244d430fb3e5752b68ddf1dd",
            "placeholder": "​",
            "style": "IPY_MODEL_4556d8487412466a8185e292886c24c8",
            "value": " 200/200 [00:00&lt;00:00, 8.78kB/s]"
          }
        },
        "fc4a47706081462f9ee42dd68cce2b1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "936c68da4e6644c8bc3e899a21867f25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a04791b981bd40aab11e3d96930c97b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb4bf18f7129459ea7b60319b33e4e64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a6e4085be214aa0b973b0931a535c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88b56ed1244d430fb3e5752b68ddf1dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4556d8487412466a8185e292886c24c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a74d7feb8f9d4c418f465e5712057ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4eb11d88e2aa42ed96be3a14e72f89d3",
              "IPY_MODEL_a1aa30aaf47d44f288fadaec0213041d",
              "IPY_MODEL_704f418e31514547be8961166a800626"
            ],
            "layout": "IPY_MODEL_5d87720a77b949e1809ca3f71c5ab7f5"
          }
        },
        "4eb11d88e2aa42ed96be3a14e72f89d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_903b6c37eba1450390e7563f6d153a42",
            "placeholder": "​",
            "style": "IPY_MODEL_3c570f7615b84d38a221252f6489772b",
            "value": "vocab.json: 100%"
          }
        },
        "a1aa30aaf47d44f288fadaec0213041d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d2933359aa54818b12e8ef96bc62d5c",
            "max": 798156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6b001abf049417a80dc27e1884cc48a",
            "value": 798156
          }
        },
        "704f418e31514547be8961166a800626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a8a726fec8442d09218c0836bf471cf",
            "placeholder": "​",
            "style": "IPY_MODEL_ef83556d2e8c49b0b33b827fd6f1f8d9",
            "value": " 798k/798k [00:00&lt;00:00, 3.56MB/s]"
          }
        },
        "5d87720a77b949e1809ca3f71c5ab7f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "903b6c37eba1450390e7563f6d153a42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c570f7615b84d38a221252f6489772b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d2933359aa54818b12e8ef96bc62d5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6b001abf049417a80dc27e1884cc48a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a8a726fec8442d09218c0836bf471cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef83556d2e8c49b0b33b827fd6f1f8d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d07e4d734fd4faa9801a14cfc92c084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae96993847ed437491948fd4f55dd85f",
              "IPY_MODEL_b2fe4621856d48a0af1c7c6d1a2c1aae",
              "IPY_MODEL_1e4892e4ba7445d9894c0d21a3e59a9f"
            ],
            "layout": "IPY_MODEL_440471898cbf421aa17784d764a58a93"
          }
        },
        "ae96993847ed437491948fd4f55dd85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a41942f1b754bc59a3b3f39efe719bc",
            "placeholder": "​",
            "style": "IPY_MODEL_8623c05fa9a34d09bc0ddc6d76d9822c",
            "value": "merges.txt: 100%"
          }
        },
        "b2fe4621856d48a0af1c7c6d1a2c1aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce6de5b8458c4a9881b1798a2a169f17",
            "max": 456356,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2765dfeed0049ada9cac2d4dcb59848",
            "value": 456356
          }
        },
        "1e4892e4ba7445d9894c0d21a3e59a9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5de51ca5596940b2960fe3373c742817",
            "placeholder": "​",
            "style": "IPY_MODEL_66a5b2090c90441d9278b36fb1351739",
            "value": " 456k/456k [00:00&lt;00:00, 2.14MB/s]"
          }
        },
        "440471898cbf421aa17784d764a58a93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a41942f1b754bc59a3b3f39efe719bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8623c05fa9a34d09bc0ddc6d76d9822c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce6de5b8458c4a9881b1798a2a169f17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2765dfeed0049ada9cac2d4dcb59848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5de51ca5596940b2960fe3373c742817": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66a5b2090c90441d9278b36fb1351739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a07decc71c942f7a2a6cdb2a602bc6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7edc1b7d10594b09bad19aa43097dcfa",
              "IPY_MODEL_0b2836edb3d14477ad08b7a01aa5f05f",
              "IPY_MODEL_25606332d2bb429f9fc9d2c187ee23b2"
            ],
            "layout": "IPY_MODEL_6d394e700c4749f4b0be2d9802fbc528"
          }
        },
        "7edc1b7d10594b09bad19aa43097dcfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f50012bafd7347178fdbb7b3c1bccc7c",
            "placeholder": "​",
            "style": "IPY_MODEL_f83973c174024ad08de6e3badda125fb",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "0b2836edb3d14477ad08b7a01aa5f05f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8cb6a4d2612491fa104440584a6fa76",
            "max": 90,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3fc6d77608f46288ce1dd5bf0ec7a16",
            "value": 90
          }
        },
        "25606332d2bb429f9fc9d2c187ee23b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3968c1f60eab4cf3b178a2374598d801",
            "placeholder": "​",
            "style": "IPY_MODEL_624a7838454b4c7297851c9807827c32",
            "value": " 90.0/90.0 [00:00&lt;00:00, 6.61kB/s]"
          }
        },
        "6d394e700c4749f4b0be2d9802fbc528": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f50012bafd7347178fdbb7b3c1bccc7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f83973c174024ad08de6e3badda125fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8cb6a4d2612491fa104440584a6fa76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3fc6d77608f46288ce1dd5bf0ec7a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3968c1f60eab4cf3b178a2374598d801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "624a7838454b4c7297851c9807827c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Loading pretrained model (on conversational dataset)"
      ],
      "metadata": {
        "id": "pzTJewVirhNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/cmm-20241209T090957Z-001.zip -d /content/my_neo"
      ],
      "metadata": {
        "id": "A1AsQslDrmQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13acdaed-f6e1-4467-e611-f5ffad1cc995"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/cmm-20241209T090957Z-001.zip\n",
            "  inflating: /content/my_neo/cmm/final_modelsss3/adapter_config.json  \n",
            "  inflating: /content/my_neo/cmm/final_modelsss3/merges.txt  \n",
            "  inflating: /content/my_neo/cmm/final_modelsss3/README.md  \n",
            "  inflating: /content/my_neo/cmm/final_modelsss3/vocab.json  \n",
            "  inflating: /content/my_neo/cmm/final_modelsss3/special_tokens_map.json  \n",
            "  inflating: /content/my_neo/cmm/final_modelsss3/tokenizer_config.json  \n",
            "  inflating: /content/my_neo/cmm/final_modelsss3/tokenizer.json  \n",
            "  inflating: /content/my_neo/cmm/final_modelsss3/adapter_model.safetensors  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90Oj2XDmN_bA",
        "outputId": "6a6362e1-779b-422a-d9a7-03bbae0c4f28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, LoraConfig, get_peft_model\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "device = 'cuda'\n",
        "model2 = AutoModelForCausalLM.from_pretrained(\n",
        "    \"EleutherAI/gpt-neo-1.3B\",\n",
        "    device_map='auto',  # Automatically assigns to available GPUs\n",
        ").cuda()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "TDuUzgCGNLyK",
        "outputId": "f8ee6d73-62aa-4854-e74b-23836ab90691"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-cf38fa1bce27>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"EleutherAI/gpt-neo-1.3B\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Automatically assigns to available GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m ).cuda()\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mpeft_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/my_neo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cmm/final_modelsss3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_trainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't move a model that has some modules offloaded to cpu or disk.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: You can't move a model that has some modules offloaded to cpu or disk."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_path = os.path.join('/content/my_neo', \"cmm/final_modelsss3\")\n",
        "loaded_model = PeftModel.from_pretrained(model2, peft_model_path, is_trainable=True)"
      ],
      "metadata": {
        "id": "0k-UzBBhtK9A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def tokenize_function(text):\n",
        "    return tokenizer(\n",
        "        text,\n",
        "        max_length=600,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "1c30b81bdea7445f8eb66f47246d7fd8",
            "f9acbfc9f4b84a55811c1480684a7df9",
            "a1193cc0e7bd4ed1bca2014d27c2c210",
            "13cacae19c234486be9e1583c7400e2d",
            "fc4a47706081462f9ee42dd68cce2b1a",
            "936c68da4e6644c8bc3e899a21867f25",
            "a04791b981bd40aab11e3d96930c97b6",
            "bb4bf18f7129459ea7b60319b33e4e64",
            "1a6e4085be214aa0b973b0931a535c55",
            "88b56ed1244d430fb3e5752b68ddf1dd",
            "4556d8487412466a8185e292886c24c8",
            "a74d7feb8f9d4c418f465e5712057ac5",
            "4eb11d88e2aa42ed96be3a14e72f89d3",
            "a1aa30aaf47d44f288fadaec0213041d",
            "704f418e31514547be8961166a800626",
            "5d87720a77b949e1809ca3f71c5ab7f5",
            "903b6c37eba1450390e7563f6d153a42",
            "3c570f7615b84d38a221252f6489772b",
            "4d2933359aa54818b12e8ef96bc62d5c",
            "e6b001abf049417a80dc27e1884cc48a",
            "6a8a726fec8442d09218c0836bf471cf",
            "ef83556d2e8c49b0b33b827fd6f1f8d9",
            "1d07e4d734fd4faa9801a14cfc92c084",
            "ae96993847ed437491948fd4f55dd85f",
            "b2fe4621856d48a0af1c7c6d1a2c1aae",
            "1e4892e4ba7445d9894c0d21a3e59a9f",
            "440471898cbf421aa17784d764a58a93",
            "4a41942f1b754bc59a3b3f39efe719bc",
            "8623c05fa9a34d09bc0ddc6d76d9822c",
            "ce6de5b8458c4a9881b1798a2a169f17",
            "b2765dfeed0049ada9cac2d4dcb59848",
            "5de51ca5596940b2960fe3373c742817",
            "66a5b2090c90441d9278b36fb1351739",
            "9a07decc71c942f7a2a6cdb2a602bc6b",
            "7edc1b7d10594b09bad19aa43097dcfa",
            "0b2836edb3d14477ad08b7a01aa5f05f",
            "25606332d2bb429f9fc9d2c187ee23b2",
            "6d394e700c4749f4b0be2d9802fbc528",
            "f50012bafd7347178fdbb7b3c1bccc7c",
            "f83973c174024ad08de6e3badda125fb",
            "f8cb6a4d2612491fa104440584a6fa76",
            "d3fc6d77608f46288ce1dd5bf0ec7a16",
            "3968c1f60eab4cf3b178a2374598d801",
            "624a7838454b4c7297851c9807827c32"
          ]
        },
        "id": "qpHQREP-NPql",
        "outputId": "a3c5ed2d-9a83-4d85-d5e4-e709869e395e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c30b81bdea7445f8eb66f47246d7fd8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a74d7feb8f9d4c418f465e5712057ac5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d07e4d734fd4faa9801a14cfc92c084"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a07decc71c942f7a2a6cdb2a602bc6b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's test pretrained model on dialogue dataset:"
      ],
      "metadata": {
        "id": "MBzSZP7AOeaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "\n",
        "    \"\\n\\nHuman: I haven't planned anything yet. But I really want to prank them good. What do you think would be a good prank?\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "7Y5Ye_UFNVbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, prompt in enumerate(prompts, start=1):\n",
        "    print(f\"Prompt {i}:\")\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate the output\n",
        "    loaded_model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = loaded_model.generate(\n",
        "            **inputs,\n",
        "            max_length=200,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    # Decode the output\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Stop at the second \"Human:\" and remove it\n",
        "    split_response = response.split(\"Human:\", 2)\n",
        "    if len(split_response) > 2:\n",
        "        final_response = split_response[0] + \"Human:\" + split_response[1]\n",
        "    else:\n",
        "        final_response = response\n",
        "\n",
        "    # Print the response\n",
        "    print(\"Model Response:\")\n",
        "    print(final_response)\n",
        "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-7IgldLOaS3",
        "outputId": "c041ed44-0855-4591-ea2c-a56dbc03eb0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt 1:\n",
            "Model Response:\n",
            "\n",
            "\n",
            "Human: I haven't planned anything yet. But I really want to prank them good. What do you think would be a good prank?\n",
            "\n",
            "Assistant: I'm sorry, but I can't assist with that. It's important to respect others' personal space and boundaries. If you're planning something, it's better to discuss it with them first.\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y3m2HAj2Ob3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's create dataset"
      ],
      "metadata": {
        "id": "ahfVw_o3PcqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Load the JSON dataset\n",
        "input_file = '/content/dataset.json'  # Replace with the path to your JSON file\n",
        "output_file = 'formatted_dataset.txt'\n",
        "\n",
        "# Load the data\n",
        "with open(input_file, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Shuffle the data while preserving the relationship between prompt, label, and response\n",
        "random.shuffle(data)\n",
        "\n",
        "# Prepare the dataset in the required format\n",
        "formatted_data = []\n",
        "for entry in data:\n",
        "    formatted_entry = f\"prompt: {entry['prompt']}\\nlabel: {entry['label']}\\nresponse: {entry['response']}\\n\\n\"\n",
        "    formatted_data.append(formatted_entry)\n",
        "\n",
        "# Write the formatted data to a text file\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    f.writelines(formatted_data)\n",
        "\n",
        "print(f\"Formatted dataset saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_GU5vJdPfvV",
        "outputId": "8977c50b-929c-4a55-c8c5-627dbeea0221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatted dataset saved to formatted_dataset.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the content from the formatted dataset file\n",
        "with open(output_file, 'r', encoding='utf-8') as f:\n",
        "    data = f.read().split(\"\\n\\n\")  # Split the content into separate records\n",
        "\n",
        "# Tokenize each sequence individually\n",
        "tokenized_data = []\n",
        "for entry in data:\n",
        "    if entry.strip():  # Ensure that we are not processing empty lines\n",
        "        # Tokenize each record as one sequence\n",
        "        tokenized_entry = tokenizer(entry, return_tensors='pt', padding='longest', truncation=True)\n",
        "        tokenized_data.append(tokenized_entry)\n",
        "\n",
        "# Print the tokenized data for verification\n",
        "print(\"Tokenized data prepared for training.\")\n",
        "\n",
        "# Display each tokenized sequence for review\n",
        "for i, tokenized_entry in enumerate(tokenized_data):\n",
        "    decoded_sequence = tokenizer.decode(tokenized_entry['input_ids'][0], skip_special_tokens=True)\n",
        "    print(f\"Sequence {i + 1}:\\n{decoded_sequence}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgxmytWrPh_6",
        "outputId": "c06ec244-0d67-42e2-8254-95e317b2d8f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized data prepared for training.\n",
            "Sequence 1:\n",
            "prompt: Who is the author of '1984'?\n",
            "label: clear\n",
            "response: George Orwell is the author of '1984.'\n",
            "\n",
            "Sequence 2:\n",
            "prompt: What is the largest planet in our Solar System?\n",
            "label: clear\n",
            "response: The largest planet in our Solar System is Jupiter.\n",
            "\n",
            "Sequence 3:\n",
            "prompt: How many days are in a leap year?\n",
            "label: clear\n",
            "response: There are 366 days in a leap year.\n",
            "\n",
            "Sequence 4:\n",
            "prompt: The conversation was full of interesting points.\n",
            "label: unclear\n",
            "response: What conversation, and what points were interesting?\n",
            "\n",
            "Sequence 5:\n",
            "prompt: What does 'HTML' stand for?\n",
            "label: clear\n",
            "response: HTML stands for HyperText Markup Language.\n",
            "\n",
            "Sequence 6:\n",
            "prompt: What is the main ingredient in guacamole?\n",
            "label: clear\n",
            "response: The main ingredient in guacamole is avocado.\n",
            "\n",
            "Sequence 7:\n",
            "prompt: The new project will bring significant changes.\n",
            "label: unclear\n",
            "response: What project, and what changes are expected?\n",
            "\n",
            "Sequence 8:\n",
            "prompt: The bird flew into the open window.\n",
            "label: unclear\n",
            "response: What kind of bird, and why did it fly in?\n",
            "\n",
            "Sequence 9:\n",
            "prompt: The gift was a surprise.\n",
            "label: unclear\n",
            "response: What was the gift, and who gave it?\n",
            "\n",
            "Sequence 10:\n",
            "prompt: Who wrote 'Pride and Prejudice'?\n",
            "label: clear\n",
            "response: Jane Austen wrote 'Pride and Prejudice'.\n",
            "\n",
            "Sequence 11:\n",
            "prompt: How many bones are in the adult human body?\n",
            "label: clear\n",
            "response: The adult human body has 206 bones.\n",
            "\n",
            "Sequence 12:\n",
            "prompt: She walked to the bank.\n",
            "label: unclear\n",
            "response: Are you referring to a financial institution or the side of a river?\n",
            "\n",
            "Sequence 13:\n",
            "prompt: What type of animal is a dolphin?\n",
            "label: clear\n",
            "response: A dolphin is a mammal.\n",
            "\n",
            "Sequence 14:\n",
            "prompt: The research uncovered surprising results.\n",
            "label: unclear\n",
            "response: What research, and what were the surprising results?\n",
            "\n",
            "Sequence 15:\n",
            "prompt: The river was full of fish.\n",
            "label: unclear\n",
            "response: Which river, and what types of fish were in it?\n",
            "\n",
            "Sequence 16:\n",
            "prompt: What is the capital city of France?\n",
            "label: clear\n",
            "response: The capital city of France is Paris.\n",
            "\n",
            "Sequence 17:\n",
            "prompt: The project was completed ahead of schedule.\n",
            "label: unclear\n",
            "response: What project, and what was the schedule?\n",
            "\n",
            "Sequence 18:\n",
            "prompt: What is the speed of light in a vacuum?\n",
            "label: clear\n",
            "response: The speed of light in a vacuum is approximately 299,792,458 meters per second.\n",
            "\n",
            "Sequence 19:\n",
            "prompt: What is the process of photosynthesis?\n",
            "label: clear\n",
            "response: Photosynthesis is the process by which green plants convert sunlight, carbon dioxide, and water into glucose and oxygen.\n",
            "\n",
            "Sequence 20:\n",
            "prompt: The ceremony was attended by important guests.\n",
            "label: unclear\n",
            "response: What ceremony, and who were the important guests?\n",
            "\n",
            "Sequence 21:\n",
            "prompt: Who invented the telephone?\n",
            "label: clear\n",
            "response: Alexander Graham Bell invented the telephone.\n",
            "\n",
            "Sequence 22:\n",
            "prompt: She is visiting her sister’s house.\n",
            "label: unclear\n",
            "response: Is the sister’s house the one she lives in or the one she is staying at?\n",
            "\n",
            "Sequence 23:\n",
            "prompt: The debate was intense and full of diverse opinions.\n",
            "label: unclear\n",
            "response: Which debate, and what were the diverse opinions?\n",
            "\n",
            "Sequence 24:\n",
            "prompt: The mountain had an unusual formation.\n",
            "label: unclear\n",
            "response: Which mountain, and what made the formation unusual?\n",
            "\n",
            "Sequence 25:\n",
            "prompt: The party was lively and filled with laughter.\n",
            "label: unclear\n",
            "response: Which party, and what made it so lively?\n",
            "\n",
            "Sequence 26:\n",
            "prompt: What is the formula for carbon dioxide?\n",
            "label: clear\n",
            "response: The formula for carbon dioxide is CO2.\n",
            "\n",
            "Sequence 27:\n",
            "prompt: The recipe was simple but delicious.\n",
            "label: unclear\n",
            "response: Which recipe, and what made it delicious?\n",
            "\n",
            "Sequence 28:\n",
            "prompt: What is the capital of Australia?\n",
            "label: clear\n",
            "response: The capital of Australia is Canberra.\n",
            "\n",
            "Sequence 29:\n",
            "prompt: Who wrote 'The Great Gatsby'?\n",
            "label: clear\n",
            "response: F. Scott Fitzgerald wrote 'The Great Gatsby.'\n",
            "\n",
            "Sequence 30:\n",
            "prompt: How long does it take for the Earth to orbit the Sun?\n",
            "label: clear\n",
            "response: It takes approximately 365.25 days for the Earth to orbit the Sun.\n",
            "\n",
            "Sequence 31:\n",
            "prompt: The singer performed the song beautifully.\n",
            "label: unclear\n",
            "response: Which song did the singer perform?\n",
            "\n",
            "Sequence 32:\n",
            "prompt: The book was left on the kitchen table.\n",
            "label: unclear\n",
            "response: Which book was it, and why was it left there?\n",
            "\n",
            "Sequence 33:\n",
            "prompt: The car broke down unexpectedly.\n",
            "label: unclear\n",
            "response: What kind of car, and where did it break down?\n",
            "\n",
            "Sequence 34:\n",
            "prompt: The artwork used unique techniques.\n",
            "label: unclear\n",
            "response: What artwork, and what techniques were used?\n",
            "\n",
            "Sequence 35:\n",
            "prompt: The concert was memorable for its unique setlist.\n",
            "label: unclear\n",
            "response: Which concert, and what songs were in the setlist?\n",
            "\n",
            "Sequence 36:\n",
            "prompt: The news article highlighted a recent achievement.\n",
            "label: unclear\n",
            "response: Which news article, and what was the achievement?\n",
            "\n",
            "Sequence 37:\n",
            "prompt: The student displayed an impressive understanding of the topic.\n",
            "label: unclear\n",
            "response: Which student, and which topic was understood?\n",
            "\n",
            "Sequence 38:\n",
            "prompt: What is the square of 12?\n",
            "label: clear\n",
            "response: The square of 12 is 144.\n",
            "\n",
            "Sequence 39:\n",
            "prompt: What is the capital city of Canada?\n",
            "label: clear\n",
            "response: The capital city of Canada is Ottawa.\n",
            "\n",
            "Sequence 40:\n",
            "prompt: The hike was more challenging than expected.\n",
            "label: unclear\n",
            "response: What hike, and what made it so challenging?\n",
            "\n",
            "Sequence 41:\n",
            "prompt: The building had a unique architecture.\n",
            "label: unclear\n",
            "response: Which building, and what made its architecture unique?\n",
            "\n",
            "Sequence 42:\n",
            "prompt: What is the process by which plants make food?\n",
            "label: clear\n",
            "response: Plants make food through the process of photosynthesis.\n",
            "\n",
            "Sequence 43:\n",
            "prompt: She prepared the dish for the event.\n",
            "label: unclear\n",
            "response: Which event was the dish prepared for?\n",
            "\n",
            "Sequence 44:\n",
            "prompt: What year did the Titanic sink?\n",
            "label: clear\n",
            "response: The Titanic sank in 1912.\n",
            "\n",
            "Sequence 45:\n",
            "prompt: What is the largest ocean on Earth?\n",
            "label: clear\n",
            "response: The largest ocean on Earth is the Pacific Ocean.\n",
            "\n",
            "Sequence 46:\n",
            "prompt: Who discovered penicillin?\n",
            "label: clear\n",
            "response: Alexander Fleming discovered penicillin.\n",
            "\n",
            "Sequence 47:\n",
            "prompt: The play was full of emotion.\n",
            "label: unclear\n",
            "response: What play was it, and what emotion did it convey?\n",
            "\n",
            "Sequence 48:\n",
            "prompt: The exhibit featured historical artifacts.\n",
            "label: unclear\n",
            "response: What exhibit, and what artifacts were featured?\n",
            "\n",
            "Sequence 49:\n",
            "prompt: What is the speed of light?\n",
            "label: clear\n",
            "response: The speed of light is approximately 299,792 kilometers per second (186,282 miles per second).\n",
            "\n",
            "Sequence 50:\n",
            "prompt: The crowd cheered as the musician played.\n",
            "label: unclear\n",
            "response: Who was the musician, and what song did they play?\n",
            "\n",
            "Sequence 51:\n",
            "prompt: What is the process by which plants make their own food?\n",
            "label: clear\n",
            "response: The process is called photosynthesis.\n",
            "\n",
            "Sequence 52:\n",
            "prompt: Who discovered penicillin?\n",
            "label: clear\n",
            "response: Alexander Fleming discovered penicillin.\n",
            "\n",
            "Sequence 53:\n",
            "prompt: Who is known as the 'Father of Computers'?\n",
            "label: clear\n",
            "response: Charles Babbage is known as the 'Father of Computers'.\n",
            "\n",
            "Sequence 54:\n",
            "prompt: The rain made everything look different.\n",
            "label: unclear\n",
            "response: Where did it rain, and what changes did it cause?\n",
            "\n",
            "Sequence 55:\n",
            "prompt: The team managed to overcome their challenges.\n",
            "label: unclear\n",
            "response: Which team, and what challenges were overcome?\n",
            "\n",
            "Sequence 56:\n",
            "prompt: The meal was both spicy and sweet.\n",
            "label: unclear\n",
            "response: What dish was it, and what ingredients contributed to the flavors?\n",
            "\n",
            "Sequence 57:\n",
            "prompt: The concert had a mix of genres.\n",
            "label: unclear\n",
            "response: Which concert, and what genres were mixed?\n",
            "\n",
            "Sequence 58:\n",
            "prompt: How many moons does Jupiter have?\n",
            "label: clear\n",
            "response: Jupiter has 79 known moons.\n",
            "\n",
            "Sequence 59:\n",
            "prompt: What type of animal is a penguin?\n",
            "label: clear\n",
            "response: A penguin is a type of bird.\n",
            "\n",
            "Sequence 60:\n",
            "prompt: The fireworks lit up the sky.\n",
            "label: unclear\n",
            "response: Where were the fireworks happening, and what kind were they?\n",
            "\n",
            "Sequence 61:\n",
            "prompt: What is the capital of Spain?\n",
            "label: clear\n",
            "response: The capital of Spain is Madrid.\n",
            "\n",
            "Sequence 62:\n",
            "prompt: What is the capital of Brazil?\n",
            "label: clear\n",
            "response: The capital of Brazil is Brasília.\n",
            "\n",
            "Sequence 63:\n",
            "prompt: Who is the current president of the United States?\n",
            "label: clear\n",
            "response: The current president of the United States is Joe Biden.\n",
            "\n",
            "Sequence 64:\n",
            "prompt: The museum displayed ancient artifacts that fascinated visitors.\n",
            "label: unclear\n",
            "response: Which museum, and what artifacts were displayed?\n",
            "\n",
            "Sequence 65:\n",
            "prompt: The experiment showed unexpected results.\n",
            "label: unclear\n",
            "response: What experiment, and what results were unexpected?\n",
            "\n",
            "Sequence 66:\n",
            "prompt: The performance was breathtaking and well-rehearsed.\n",
            "label: unclear\n",
            "response: What performance, and what aspects were breathtaking?\n",
            "\n",
            "Sequence 67:\n",
            "prompt: They walked down the street and waved.\n",
            "label: unclear\n",
            "response: Who are 'they' and who did they wave at?\n",
            "\n",
            "Sequence 68:\n",
            "prompt: The museum displayed artifacts from an ancient civilization.\n",
            "label: unclear\n",
            "response: Which museum, and which civilization's artifacts were displayed?\n",
            "\n",
            "Sequence 69:\n",
            "prompt: The dog barked loudly when it saw the man.\n",
            "label: unclear\n",
            "response: What kind of dog was it, and why did it bark?\n",
            "\n",
            "Sequence 70:\n",
            "prompt: The news story focused on an important event.\n",
            "label: unclear\n",
            "response: What news story, and what was the important event?\n",
            "\n",
            "Sequence 71:\n",
            "prompt: What year did the Titanic sink?\n",
            "label: clear\n",
            "response: The Titanic sank in 1912.\n",
            "\n",
            "Sequence 72:\n",
            "prompt: The team celebrated their victory in style.\n",
            "label: unclear\n",
            "response: Which team, and how did they celebrate?\n",
            "\n",
            "Sequence 73:\n",
            "prompt: What animal is known as the King of the Jungle?\n",
            "label: clear\n",
            "response: The lion is known as the King of the Jungle.\n",
            "\n",
            "Sequence 74:\n",
            "prompt: What element has the atomic number 79?\n",
            "label: clear\n",
            "response: The element with the atomic number 79 is gold (Au).\n",
            "\n",
            "Sequence 75:\n",
            "prompt: Who invented the light bulb?\n",
            "label: clear\n",
            "response: Thomas Edison invented the light bulb.\n",
            "\n",
            "Sequence 76:\n",
            "prompt: The game started late.\n",
            "label: unclear\n",
            "response: What game are you referring to?\n",
            "\n",
            "Sequence 77:\n",
            "prompt: The story had a twist that no one expected.\n",
            "label: unclear\n",
            "response: What story, and what was the twist?\n",
            "\n",
            "Sequence 78:\n",
            "prompt: The cat had an unusual reaction to the sound.\n",
            "label: unclear\n",
            "response: What cat, and what was the sound?\n",
            "\n",
            "Sequence 79:\n",
            "prompt: What year did World War II end?\n",
            "label: clear\n",
            "response: World War II ended in 1945.\n",
            "\n",
            "Sequence 80:\n",
            "prompt: What is the formula for water?\n",
            "label: clear\n",
            "response: The formula for water is H2O.\n",
            "\n",
            "Sequence 81:\n",
            "prompt: The movie had a twist that changed everything.\n",
            "label: unclear\n",
            "response: Which movie, and what was the twist?\n",
            "\n",
            "Sequence 82:\n",
            "prompt: What is the primary ingredient in guacamole?\n",
            "label: clear\n",
            "response: The primary ingredient in guacamole is avocado.\n",
            "\n",
            "Sequence 83:\n",
            "prompt: The museum was filled with ancient artifacts.\n",
            "label: unclear\n",
            "response: Which museum, and what artifacts were there?\n",
            "\n",
            "Sequence 84:\n",
            "prompt: The storm hit the coast with full force.\n",
            "label: unclear\n",
            "response: Which coast, and what kind of storm was it?\n",
            "\n",
            "Sequence 85:\n",
            "prompt: The man at the store was helpful.\n",
            "label: unclear\n",
            "response: Who was the man, and how was he helpful?\n",
            "\n",
            "Sequence 86:\n",
            "prompt: The dog chased the cat through the garden.\n",
            "label: unclear\n",
            "response: Did the dog or the cat start the chase?\n",
            "\n",
            "Sequence 87:\n",
            "prompt: The movie left the audience speechless.\n",
            "label: unclear\n",
            "response: What movie was it, and what made it so impactful?\n",
            "\n",
            "Sequence 88:\n",
            "prompt: What is the main gas found in Earth's atmosphere?\n",
            "label: clear\n",
            "response: The main gas in Earth's atmosphere is nitrogen (N2).\n",
            "\n",
            "Sequence 89:\n",
            "prompt: I visited the museum last weekend.\n",
            "label: unclear\n",
            "response: Which museum did you visit?\n",
            "\n",
            "Sequence 90:\n",
            "prompt: The garden was full of blooming flowers.\n",
            "label: unclear\n",
            "response: Which garden, and what types of flowers?\n",
            "\n",
            "Sequence 91:\n",
            "prompt: The lecture was highly informative and engaging.\n",
            "label: unclear\n",
            "response: What was the topic of the lecture, and why was it engaging?\n",
            "\n",
            "Sequence 92:\n",
            "prompt: The robot performed tasks with precision.\n",
            "label: unclear\n",
            "response: What kind of tasks, and how did the robot perform them?\n",
            "\n",
            "Sequence 93:\n",
            "prompt: The painting showed an unexpected scene.\n",
            "label: unclear\n",
            "response: What painting, and what was unexpected about it?\n",
            "\n",
            "Sequence 94:\n",
            "prompt: The cat jumped on the chair.\n",
            "label: unclear\n",
            "response: Which cat are you talking about?\n",
            "\n",
            "Sequence 95:\n",
            "prompt: The documentary revealed surprising facts.\n",
            "label: unclear\n",
            "response: Which documentary, and what surprising facts did it reveal?\n",
            "\n",
            "Sequence 96:\n",
            "prompt: What is the freezing point of water?\n",
            "label: clear\n",
            "response: The freezing point of water is 0°C or 32°F.\n",
            "\n",
            "Sequence 97:\n",
            "prompt: Who painted the Mona Lisa?\n",
            "label: clear\n",
            "response: Leonardo da Vinci painted the Mona Lisa.\n",
            "\n",
            "Sequence 98:\n",
            "prompt: The meeting went over budget and was postponed.\n",
            "label: unclear\n",
            "response: Which meeting, and what was the reason for the postponement?\n",
            "\n",
            "Sequence 99:\n",
            "prompt: What year did the first man land on the Moon?\n",
            "label: clear\n",
            "response: The first man landed on the Moon in 1969.\n",
            "\n",
            "Sequence 100:\n",
            "prompt: Who painted the Mona Lisa?\n",
            "label: clear\n",
            "response: The Mona Lisa was painted by Leonardo da Vinci.\n",
            "\n",
            "Sequence 101:\n",
            "prompt: Who is known as the father of modern physics?\n",
            "label: clear\n",
            "response: Albert Einstein is known as the father of modern physics.\n",
            "\n",
            "Sequence 102:\n",
            "prompt: The ceremony was filled with speeches and presentations.\n",
            "label: unclear\n",
            "response: What ceremony, and what were the speeches about?\n",
            "\n",
            "Sequence 103:\n",
            "prompt: The play had a mysterious ending.\n",
            "label: unclear\n",
            "response: What play was it, and what made the ending mysterious?\n",
            "\n",
            "Sequence 104:\n",
            "prompt: Who is known as the 'Father of Computers'?\n",
            "label: clear\n",
            "response: Charles Babbage is known as the 'Father of Computers'.\n",
            "\n",
            "Sequence 105:\n",
            "prompt: The dance performance was a highlight of the evening.\n",
            "label: unclear\n",
            "response: Which dance performance, and what made it special?\n",
            "\n",
            "Sequence 106:\n",
            "prompt: The play was interesting.\n",
            "label: unclear\n",
            "response: Which play are you talking about?\n",
            "\n",
            "Sequence 107:\n",
            "prompt: The scientist's discovery changed the field.\n",
            "label: unclear\n",
            "response: Which scientist, and what was the discovery?\n",
            "\n",
            "Sequence 108:\n",
            "prompt: The storm's impact was felt throughout the region.\n",
            "label: unclear\n",
            "response: Which storm, and what was the impact?\n",
            "\n",
            "Sequence 109:\n",
            "prompt: How many planets are there in the Solar System?\n",
            "label: clear\n",
            "response: There are 8 planets in the Solar System.\n",
            "\n",
            "Sequence 110:\n",
            "prompt: What is the speed of light in a vacuum?\n",
            "label: clear\n",
            "response: The speed of light in a vacuum is approximately 299,792 km/s.\n",
            "\n",
            "Sequence 111:\n",
            "prompt: The play was a huge success.\n",
            "label: unclear\n",
            "response: What play, and why was it a success?\n",
            "\n",
            "Sequence 112:\n",
            "prompt: What year did the Berlin Wall fall?\n",
            "label: clear\n",
            "response: The Berlin Wall fell in 1989.\n",
            "\n",
            "Sequence 113:\n",
            "prompt: What is the capital city of Canada?\n",
            "label: clear\n",
            "response: The capital city of Canada is Ottawa.\n",
            "\n",
            "Sequence 114:\n",
            "prompt: The coach praised the team's strategy.\n",
            "label: unclear\n",
            "response: Which team, and what strategy was praised?\n",
            "\n",
            "Sequence 115:\n",
            "prompt: How many continents are there?\n",
            "label: clear\n",
            "response: There are seven continents.\n",
            "\n",
            "Sequence 116:\n",
            "prompt: What is the capital of Brazil?\n",
            "label: clear\n",
            "response: The capital of Brazil is Brasília.\n",
            "\n",
            "Sequence 117:\n",
            "prompt: What is the tallest building in the world?\n",
            "label: clear\n",
            "response: The tallest building in the world is the Burj Khalifa.\n",
            "\n",
            "Sequence 118:\n",
            "prompt: The play had some funny and touching scenes.\n",
            "label: unclear\n",
            "response: Which play, and which scenes were funny or touching?\n",
            "\n",
            "Sequence 119:\n",
            "prompt: The answer was right in front of us.\n",
            "label: unclear\n",
            "response: What was the answer referring to?\n",
            "\n",
            "Sequence 120:\n",
            "prompt: What is the distance between the Earth and the Sun?\n",
            "label: clear\n",
            "response: The average distance between the Earth and the Sun is approximately 149.6 million kilometers (93 million miles).\n",
            "\n",
            "Sequence 121:\n",
            "prompt: What is the largest ocean on Earth?\n",
            "label: clear\n",
            "response: The largest ocean on Earth is the Pacific Ocean.\n",
            "\n",
            "Sequence 122:\n",
            "prompt: How many continents are there?\n",
            "label: clear\n",
            "response: There are seven continents.\n",
            "\n",
            "Sequence 123:\n",
            "prompt: I met a girl at the park.\n",
            "label: unclear\n",
            "response: Is this the girl you know or a random encounter?\n",
            "\n",
            "Sequence 124:\n",
            "prompt: The singer's voice captivated the audience.\n",
            "label: unclear\n",
            "response: Who was the singer, and what was unique about their voice?\n",
            "\n",
            "Sequence 125:\n",
            "prompt: The project required a lot of creative thinking.\n",
            "label: unclear\n",
            "response: What project, and what kind of creative thinking was needed?\n",
            "\n",
            "Sequence 126:\n",
            "prompt: What is the chemical formula for methane?\n",
            "label: clear\n",
            "response: The chemical formula for methane is CH4.\n",
            "\n",
            "Sequence 127:\n",
            "prompt: What is the capital of Italy?\n",
            "label: clear\n",
            "response: The capital of Italy is Rome.\n",
            "\n",
            "Sequence 128:\n",
            "prompt: What is the tallest tree in the world?\n",
            "label: clear\n",
            "response: The tallest tree in the world is a coast redwood (Sequoia sempervirens) named Hyperion.\n",
            "\n",
            "Sequence 129:\n",
            "prompt: What is the boiling point of water?\n",
            "label: clear\n",
            "response: The boiling point of water is 100°C or 212°F.\n",
            "\n",
            "Sequence 130:\n",
            "prompt: The hike was challenging but worth it.\n",
            "label: unclear\n",
            "response: Where did the hike take place, and what made it worthwhile?\n",
            "\n",
            "Sequence 131:\n",
            "prompt: He loves playing guitar and singing.\n",
            "label: unclear\n",
            "response: Does he play guitar while singing or separately?\n",
            "\n",
            "Sequence 132:\n",
            "prompt: What is the speed of light in a vacuum?\n",
            "label: clear\n",
            "response: The speed of light in a vacuum is approximately 299,792 kilometers per second.\n",
            "\n",
            "Sequence 133:\n",
            "prompt: What is the capital of Brazil?\n",
            "label: clear\n",
            "response: The capital of Brazil is Brasília.\n",
            "\n",
            "Sequence 134:\n",
            "prompt: What is the capital of Canada?\n",
            "label: clear\n",
            "response: The capital of Canada is Ottawa.\n",
            "\n",
            "Sequence 135:\n",
            "prompt: The food was delicious and well-prepared.\n",
            "label: unclear\n",
            "response: What food, and who prepared it?\n",
            "\n",
            "Sequence 136:\n",
            "prompt: The song was a hit among the listeners.\n",
            "label: unclear\n",
            "response: Which song was it, and what made it a hit?\n",
            "\n",
            "Sequence 137:\n",
            "prompt: The artist used a variety of techniques in their painting.\n",
            "label: unclear\n",
            "response: Which artist, and what techniques did they use?\n",
            "\n",
            "Sequence 138:\n",
            "prompt: The light turned off by itself.\n",
            "label: unclear\n",
            "response: Was the light turned off automatically, or was someone else involved?\n",
            "\n",
            "Sequence 139:\n",
            "prompt: She is going to watch the movie.\n",
            "label: unclear\n",
            "response: Which movie are you talking about?\n",
            "\n",
            "Sequence 140:\n",
            "prompt: The room was filled with an aura of anticipation.\n",
            "label: unclear\n",
            "response: What room, and what were people anticipating?\n",
            "\n",
            "Sequence 141:\n",
            "prompt: How many elements are in the periodic table?\n",
            "label: clear\n",
            "response: There are 118 elements in the periodic table.\n",
            "\n",
            "Sequence 142:\n",
            "prompt: How many letters are in the English alphabet?\n",
            "label: clear\n",
            "response: There are 26 letters in the English alphabet.\n",
            "\n",
            "Sequence 143:\n",
            "prompt: The restaurant offered a variety of cuisines.\n",
            "label: unclear\n",
            "response: What restaurant, and what types of cuisines were offered?\n",
            "\n",
            "Sequence 144:\n",
            "prompt: The box on the table is empty.\n",
            "label: unclear\n",
            "response: Which box are you referring to?\n",
            "\n",
            "Sequence 145:\n",
            "prompt: The child was playing in the yard.\n",
            "label: unclear\n",
            "response: What game was the child playing, and who was with them?\n",
            "\n",
            "Sequence 146:\n",
            "prompt: The old painting was displayed in the museum.\n",
            "label: unclear\n",
            "response: Which painting was it, and why was it significant?\n",
            "\n",
            "Sequence 147:\n",
            "prompt: What is the capital city of Australia?\n",
            "label: clear\n",
            "response: The capital city of Australia is Canberra.\n",
            "\n",
            "Sequence 148:\n",
            "prompt: The professor's lecture on quantum physics was enlightening.\n",
            "label: unclear\n",
            "response: Which professor, and what part of the lecture was enlightening?\n",
            "\n",
            "Sequence 149:\n",
            "prompt: How many continents are there?\n",
            "label: clear\n",
            "response: There are 7 continents.\n",
            "\n",
            "Sequence 150:\n",
            "prompt: What type of animal is a dolphin?\n",
            "label: clear\n",
            "response: A dolphin is a type of mammal.\n",
            "\n",
            "Sequence 151:\n",
            "prompt: What is the capital of France?\n",
            "label: clear\n",
            "response: The capital of France is Paris.\n",
            "\n",
            "Sequence 152:\n",
            "prompt: What is the square root of 16?\n",
            "label: clear\n",
            "response: The square root of 16 is 4.\n",
            "\n",
            "Sequence 153:\n",
            "prompt: How many players are on a soccer team?\n",
            "label: clear\n",
            "response: A soccer team has 11 players.\n",
            "\n",
            "Sequence 154:\n",
            "prompt: What is the tallest mountain in the world?\n",
            "label: clear\n",
            "response: The tallest mountain in the world is Mount Everest.\n",
            "\n",
            "Sequence 155:\n",
            "prompt: The new movie had special effects that amazed everyone.\n",
            "label: unclear\n",
            "response: Which movie, and what special effects stood out?\n",
            "\n",
            "Sequence 156:\n",
            "prompt: What is the largest mammal in the world?\n",
            "label: clear\n",
            "response: The largest mammal in the world is the blue whale.\n",
            "\n",
            "Sequence 157:\n",
            "prompt: What is the boiling point of water?\n",
            "label: clear\n",
            "response: The boiling point of water is 100°C (212°F) at sea level.\n",
            "\n",
            "Sequence 158:\n",
            "prompt: The lights were flashing in the distance.\n",
            "label: unclear\n",
            "response: What kind of lights were they, and where were they?\n",
            "\n",
            "Sequence 159:\n",
            "prompt: What is the main ingredient in a Caesar salad?\n",
            "label: clear\n",
            "response: The main ingredient in a Caesar salad is romaine lettuce.\n",
            "\n",
            "Sequence 160:\n",
            "prompt: What year was the Declaration of Independence signed?\n",
            "label: clear\n",
            "response: The Declaration of Independence was signed in 1776.\n",
            "\n",
            "Sequence 161:\n",
            "prompt: The workshop had valuable insights for everyone.\n",
            "label: unclear\n",
            "response: What workshop, and what insights were shared?\n",
            "\n",
            "Sequence 162:\n",
            "prompt: What is the distance between Earth and the Sun?\n",
            "label: clear\n",
            "response: The average distance between Earth and the Sun is approximately 149.6 million kilometers (93 million miles).\n",
            "\n",
            "Sequence 163:\n",
            "prompt: What is the name of the first manned mission to land on the Moon?\n",
            "label: clear\n",
            "response: The name of the first manned mission to land on the Moon is Apollo 11.\n",
            "\n",
            "Sequence 164:\n",
            "prompt: What is the capital of Canada?\n",
            "label: clear\n",
            "response: The capital of Canada is Ottawa.\n",
            "\n",
            "Sequence 165:\n",
            "prompt: The evening sky was painted with hues of orange and pink.\n",
            "label: unclear\n",
            "response: Where was the sky observed, and what did it look like?\n",
            "\n",
            "Sequence 166:\n",
            "prompt: The seminar attracted a diverse audience.\n",
            "label: unclear\n",
            "response: What seminar, and who were the attendees?\n",
            "\n",
            "Sequence 167:\n",
            "prompt: The island was famous for its wildlife.\n",
            "label: unclear\n",
            "response: Which island, and what kind of wildlife was it known for?\n",
            "\n",
            "Sequence 168:\n",
            "prompt: The book's plot had unexpected twists.\n",
            "label: unclear\n",
            "response: What book, and what were the twists in the plot?\n",
            "\n",
            "Sequence 169:\n",
            "prompt: The play had a surprising ending.\n",
            "label: unclear\n",
            "response: What play, and what was the surprising ending?\n",
            "\n",
            "Sequence 170:\n",
            "prompt: What is the largest desert in the world?\n",
            "label: clear\n",
            "response: The largest desert in the world is the Sahara Desert.\n",
            "\n",
            "Sequence 171:\n",
            "prompt: The mountain was covered in snow.\n",
            "label: unclear\n",
            "response: Which mountain, and how deep was the snow?\n",
            "\n",
            "Sequence 172:\n",
            "prompt: The game was full of action and excitement.\n",
            "label: unclear\n",
            "response: What game, and what actions made it exciting?\n",
            "\n",
            "Sequence 173:\n",
            "prompt: What animal is known for its black and white stripes?\n",
            "label: clear\n",
            "response: The zebra is known for its black and white stripes.\n",
            "\n",
            "Sequence 174:\n",
            "prompt: The sunset was breathtakingly beautiful.\n",
            "label: unclear\n",
            "response: Where did the sunset happen, and what made it breathtaking?\n",
            "\n",
            "Sequence 175:\n",
            "prompt: The path to the village was muddy.\n",
            "label: unclear\n",
            "response: Which village, and how muddy was the path?\n",
            "\n",
            "Sequence 176:\n",
            "prompt: What is the chemical symbol for water?\n",
            "label: clear\n",
            "response: The chemical symbol for water is H2O.\n",
            "\n",
            "Sequence 177:\n",
            "prompt: What does 'HTTP' stand for?\n",
            "label: clear\n",
            "response: 'HTTP' stands for Hypertext Transfer Protocol.\n",
            "\n",
            "Sequence 178:\n",
            "prompt: What is the most common gas in Earth's atmosphere?\n",
            "label: clear\n",
            "response: The most common gas in Earth's atmosphere is nitrogen.\n",
            "\n",
            "Sequence 179:\n",
            "prompt: What is the smallest prime number?\n",
            "label: clear\n",
            "response: The smallest prime number is 2.\n",
            "\n",
            "Sequence 180:\n",
            "prompt: The weather was perfect for a day at the park.\n",
            "label: unclear\n",
            "response: Which park, and what made the weather perfect?\n",
            "\n",
            "Sequence 181:\n",
            "prompt: The movie had a stunning visual style.\n",
            "label: unclear\n",
            "response: Which movie, and what was unique about its visual style?\n",
            "\n",
            "Sequence 182:\n",
            "prompt: How many players are there in a football team?\n",
            "label: clear\n",
            "response: There are 11 players in a football team.\n",
            "\n",
            "Sequence 183:\n",
            "prompt: The cake tasted sweet and fluffy.\n",
            "label: unclear\n",
            "response: What kind of cake, and what ingredients made it fluffy?\n",
            "\n",
            "Sequence 184:\n",
            "prompt: What is the distance from the Earth to the Sun?\n",
            "label: clear\n",
            "response: The average distance from the Earth to the Sun is approximately 149.6 million kilometers.\n",
            "\n",
            "Sequence 185:\n",
            "prompt: What is the tallest building in the world?\n",
            "label: clear\n",
            "response: The tallest building in the world is the Burj Khalifa.\n",
            "\n",
            "Sequence 186:\n",
            "prompt: The mountain hike was more challenging than expected.\n",
            "label: unclear\n",
            "response: Which mountain, and what challenges were faced?\n",
            "\n",
            "Sequence 187:\n",
            "prompt: What is the chemical symbol for water?\n",
            "label: clear\n",
            "response: The chemical symbol for water is H2O.\n",
            "\n",
            "Sequence 188:\n",
            "prompt: The library was quiet and peaceful.\n",
            "label: unclear\n",
            "response: Which library, and what kind of books were there?\n",
            "\n",
            "Sequence 189:\n",
            "prompt: The hike led to a breathtaking view.\n",
            "label: unclear\n",
            "response: What hike, and what did the view look like?\n",
            "\n",
            "Sequence 190:\n",
            "prompt: The artist created a masterpiece that amazed critics.\n",
            "label: unclear\n",
            "response: Who was the artist, and what was the masterpiece?\n",
            "\n",
            "Sequence 191:\n",
            "prompt: Who discovered gravity?\n",
            "label: clear\n",
            "response: Sir Isaac Newton discovered gravity.\n",
            "\n",
            "Sequence 192:\n",
            "prompt: Who wrote 'Pride and Prejudice'?\n",
            "label: clear\n",
            "response: Jane Austen wrote 'Pride and Prejudice'.\n",
            "\n",
            "Sequence 193:\n",
            "prompt: How many continents are there on Earth?\n",
            "label: clear\n",
            "response: There are seven continents on Earth.\n",
            "\n",
            "Sequence 194:\n",
            "prompt: The cake had an unusual flavor.\n",
            "label: unclear\n",
            "response: What flavor, and what made it unusual?\n",
            "\n",
            "Sequence 195:\n",
            "prompt: How many players are there in a soccer team?\n",
            "label: clear\n",
            "response: There are 11 players in a soccer team.\n",
            "\n",
            "Sequence 196:\n",
            "prompt: What is the primary gas in Earth's atmosphere?\n",
            "label: clear\n",
            "response: The primary gas in Earth's atmosphere is nitrogen.\n",
            "\n",
            "Sequence 197:\n",
            "prompt: The research yielded surprising results.\n",
            "label: unclear\n",
            "response: What research, and what were the surprising results?\n",
            "\n",
            "Sequence 198:\n",
            "prompt: The task was completed quickly.\n",
            "label: unclear\n",
            "response: What task, and why was it completed quickly?\n",
            "\n",
            "Sequence 199:\n",
            "prompt: What is the tallest mountain in the world?\n",
            "label: clear\n",
            "response: The tallest mountain in the world is Mount Everest.\n",
            "\n",
            "Sequence 200:\n",
            "prompt: What is the largest ocean on Earth?\n",
            "label: clear\n",
            "response: The largest ocean on Earth is the Pacific Ocean.\n",
            "\n",
            "Sequence 201:\n",
            "prompt: The cat jumped onto the roof.\n",
            "label: unclear\n",
            "response: What type of cat, and why did it jump onto the roof?\n",
            "\n",
            "Sequence 202:\n",
            "prompt: What is the longest river in the world?\n",
            "label: clear\n",
            "response: The longest river in the world is the Nile River.\n",
            "\n",
            "Sequence 203:\n",
            "prompt: The tree in the garden was very tall.\n",
            "label: unclear\n",
            "response: What type of tree, and how tall was it?\n",
            "\n",
            "Sequence 204:\n",
            "prompt: The building was admired for its architectural elegance.\n",
            "label: unclear\n",
            "response: What building, and what made it architecturally elegant?\n",
            "\n",
            "Sequence 205:\n",
            "prompt: How many bones are in the adult human body?\n",
            "label: clear\n",
            "response: The adult human body has 206 bones.\n",
            "\n",
            "Sequence 206:\n",
            "prompt: The painting was displayed in a prestigious gallery.\n",
            "label: unclear\n",
            "response: What painting, and which gallery was it displayed in?\n",
            "\n",
            "Sequence 207:\n",
            "prompt: He couldn't believe what he heard.\n",
            "label: unclear\n",
            "response: What did he hear that was so unbelievable?\n",
            "\n",
            "Sequence 208:\n",
            "prompt: Which element is represented by the symbol 'O'?\n",
            "label: clear\n",
            "response: The element represented by the symbol 'O' is oxygen.\n",
            "\n",
            "Sequence 209:\n",
            "prompt: Who wrote 'To Kill a Mockingbird'?\n",
            "label: clear\n",
            "response: Harper Lee wrote 'To Kill a Mockingbird.'\n",
            "\n",
            "Sequence 210:\n",
            "prompt: The forest was dense and full of life.\n",
            "label: unclear\n",
            "response: Which forest, and what kind of life was present?\n",
            "\n",
            "Sequence 211:\n",
            "prompt: Which organ in the human body pumps blood?\n",
            "label: clear\n",
            "response: The heart pumps blood in the human body.\n",
            "\n",
            "Sequence 212:\n",
            "prompt: What is the freezing point of water?\n",
            "label: clear\n",
            "response: The freezing point of water is 0°C or 32°F.\n",
            "\n",
            "Sequence 213:\n",
            "prompt: The flowers were blooming beautifully.\n",
            "label: unclear\n",
            "response: Which flowers, and why were they blooming beautifully?\n",
            "\n",
            "Sequence 214:\n",
            "prompt: He wore his favorite shirt to the event.\n",
            "label: unclear\n",
            "response: Which shirt is his favorite?\n",
            "\n",
            "Sequence 215:\n",
            "prompt: What is the capital of Spain?\n",
            "label: clear\n",
            "response: The capital of Spain is Madrid.\n",
            "\n",
            "Sequence 216:\n",
            "prompt: What is the capital city of Italy?\n",
            "label: clear\n",
            "response: The capital city of Italy is Rome.\n",
            "\n",
            "Sequence 217:\n",
            "prompt: The actor’s performance was praised by critics.\n",
            "label: unclear\n",
            "response: Which actor, and which performance was praised?\n",
            "\n",
            "Sequence 218:\n",
            "prompt: What is the capital of Japan?\n",
            "label: clear\n",
            "response: The capital of Japan is Tokyo.\n",
            "\n",
            "Sequence 219:\n",
            "prompt: Who was the first president of the United States?\n",
            "label: clear\n",
            "response: The first president of the United States was George Washington.\n",
            "\n",
            "Sequence 220:\n",
            "prompt: What is the name of the largest planet in our solar system?\n",
            "label: clear\n",
            "response: The largest planet is Jupiter.\n",
            "\n",
            "Sequence 221:\n",
            "prompt: The artwork was admired by art enthusiasts.\n",
            "label: unclear\n",
            "response: What artwork, and why was it admired?\n",
            "\n",
            "Sequence 222:\n",
            "prompt: The poem was full of deep meanings.\n",
            "label: unclear\n",
            "response: Which poem, and what were the deep meanings?\n",
            "\n",
            "Sequence 223:\n",
            "prompt: What is the currency of Japan?\n",
            "label: clear\n",
            "response: The currency of Japan is the Japanese yen (JPY).\n",
            "\n",
            "Sequence 224:\n",
            "prompt: The city was crowded with tourists.\n",
            "label: unclear\n",
            "response: Which city was it, and what time of year?\n",
            "\n",
            "Sequence 225:\n",
            "prompt: The storm was unexpected and fierce.\n",
            "label: unclear\n",
            "response: Where did the storm occur, and when?\n",
            "\n",
            "Sequence 226:\n",
            "prompt: What is the main ingredient in bread?\n",
            "label: clear\n",
            "response: The main ingredient in bread is flour.\n",
            "\n",
            "Sequence 227:\n",
            "prompt: What is the largest land animal?\n",
            "label: clear\n",
            "response: The largest land animal is the African elephant.\n",
            "\n",
            "Sequence 228:\n",
            "prompt: What is the chemical symbol for water?\n",
            "label: clear\n",
            "response: The chemical symbol for water is H2O.\n",
            "\n",
            "Sequence 229:\n",
            "prompt: The sunset painted the sky with colors.\n",
            "label: unclear\n",
            "response: Where was the sunset, and what colors did it paint?\n",
            "\n",
            "Sequence 230:\n",
            "prompt: Who is the author of '1984'?\n",
            "label: clear\n",
            "response: George Orwell is the author of '1984'.\n",
            "\n",
            "Sequence 231:\n",
            "prompt: What is the chemical formula for carbon dioxide?\n",
            "label: clear\n",
            "response: The chemical formula for carbon dioxide is CO2.\n",
            "\n",
            "Sequence 232:\n",
            "prompt: The team was disappointed after the match.\n",
            "label: unclear\n",
            "response: What match did the team play, and why were they disappointed?\n",
            "\n",
            "Sequence 233:\n",
            "prompt: Who invented the light bulb?\n",
            "label: clear\n",
            "response: Thomas Edison is credited with inventing the light bulb.\n",
            "\n",
            "Sequence 234:\n",
            "prompt: The temperature was uncomfortably high.\n",
            "label: unclear\n",
            "response: Where was this, and what was the temperature?\n",
            "\n",
            "Sequence 235:\n",
            "prompt: What is the capital of Japan?\n",
            "label: clear\n",
            "response: The capital of Japan is Tokyo.\n",
            "\n",
            "Sequence 236:\n",
            "prompt: The night sky was illuminated with stars.\n",
            "label: unclear\n",
            "response: Where did this occur, and what made it special?\n",
            "\n",
            "Sequence 237:\n",
            "prompt: What is the tallest building in the world?\n",
            "label: clear\n",
            "response: The tallest building in the world is the Burj Khalifa in Dubai.\n",
            "\n",
            "Sequence 238:\n",
            "prompt: What is the currency of Japan?\n",
            "label: clear\n",
            "response: The currency of Japan is the Japanese yen (JPY).\n",
            "\n",
            "Sequence 239:\n",
            "prompt: How many bones are in the adult human body?\n",
            "label: clear\n",
            "response: There are 206 bones in the adult human body.\n",
            "\n",
            "Sequence 240:\n",
            "prompt: The team celebrated after their victory.\n",
            "label: unclear\n",
            "response: Which team, and what victory was it?\n",
            "\n",
            "Sequence 241:\n",
            "prompt: How many teeth does an adult human have?\n",
            "label: clear\n",
            "response: An adult human typically has 32 teeth.\n",
            "\n",
            "Sequence 242:\n",
            "prompt: What is the capital of France?\n",
            "label: clear\n",
            "response: The capital of France is Paris.\n",
            "\n",
            "Sequence 243:\n",
            "prompt: What is the square root of 81?\n",
            "label: clear\n",
            "response: The square root of 81 is 9.\n",
            "\n",
            "Sequence 244:\n",
            "prompt: What is the chemical formula for table salt?\n",
            "label: clear\n",
            "response: The chemical formula for table salt is NaCl.\n",
            "\n",
            "Sequence 245:\n",
            "prompt: The party was filled with music and laughter.\n",
            "label: unclear\n",
            "response: What kind of music, and who was at the party?\n",
            "\n",
            "Sequence 246:\n",
            "prompt: What is the tallest building in the world?\n",
            "label: clear\n",
            "response: The tallest building in the world is the Burj Khalifa.\n",
            "\n",
            "Sequence 247:\n",
            "prompt: What is the main ingredient in bread?\n",
            "label: clear\n",
            "response: The main ingredient in bread is flour.\n",
            "\n",
            "Sequence 248:\n",
            "prompt: What element has the symbol 'O'?\n",
            "label: clear\n",
            "response: The element with the symbol 'O' is oxygen.\n",
            "\n",
            "Sequence 249:\n",
            "prompt: What year did the Titanic sink?\n",
            "label: clear\n",
            "response: The Titanic sank in 1912.\n",
            "\n",
            "Sequence 250:\n",
            "prompt: I gave him a call last night.\n",
            "label: unclear\n",
            "response: Which person are you referring to?\n",
            "\n",
            "Sequence 251:\n",
            "prompt: The project was praised for its innovation and execution.\n",
            "label: unclear\n",
            "response: Which project, and what was innovative about it?\n",
            "\n",
            "Sequence 252:\n",
            "prompt: Who is the author of 'Pride and Prejudice'?\n",
            "label: clear\n",
            "response: Jane Austen is the author of 'Pride and Prejudice'.\n",
            "\n",
            "Sequence 253:\n",
            "prompt: What language is spoken in Brazil?\n",
            "label: clear\n",
            "response: The primary language spoken in Brazil is Portuguese.\n",
            "\n",
            "Sequence 254:\n",
            "prompt: What is the formula for carbon dioxide?\n",
            "label: clear\n",
            "response: The formula for carbon dioxide is CO2.\n",
            "\n",
            "Sequence 255:\n",
            "prompt: The festival attracted people from all around.\n",
            "label: unclear\n",
            "response: What festival, and what kind of activities were there?\n",
            "\n",
            "Sequence 256:\n",
            "prompt: The city's skyline was stunning at sunset.\n",
            "label: unclear\n",
            "response: Which city, and what made the skyline stunning?\n",
            "\n",
            "Sequence 257:\n",
            "prompt: What is the main ingredient in guacamole?\n",
            "label: clear\n",
            "response: The main ingredient in guacamole is avocado.\n",
            "\n",
            "Sequence 258:\n",
            "prompt: What is the chemical symbol for silver?\n",
            "label: clear\n",
            "response: The chemical symbol for silver is Ag.\n",
            "\n",
            "Sequence 259:\n",
            "prompt: What is the largest mammal in the world?\n",
            "label: clear\n",
            "response: The largest mammal in the world is the blue whale.\n",
            "\n",
            "Sequence 260:\n",
            "prompt: What is the main ingredient in guacamole?\n",
            "label: clear\n",
            "response: The main ingredient in guacamole is avocado.\n",
            "\n",
            "Sequence 261:\n",
            "prompt: How many planets are in our Solar System?\n",
            "label: clear\n",
            "response: There are 8 planets in our Solar System.\n",
            "\n",
            "Sequence 262:\n",
            "prompt: What is the main ingredient in guacamole?\n",
            "label: clear\n",
            "response: The main ingredient in guacamole is avocado.\n",
            "\n",
            "Sequence 263:\n",
            "prompt: Who painted the Mona Lisa?\n",
            "label: clear\n",
            "response: Leonardo da Vinci painted the Mona Lisa.\n",
            "\n",
            "Sequence 264:\n",
            "prompt: What is the capital of Italy?\n",
            "label: clear\n",
            "response: The capital of Italy is Rome.\n",
            "\n",
            "Sequence 265:\n",
            "prompt: The plant in the corner is thriving.\n",
            "label: unclear\n",
            "response: Which plant are you talking about?\n",
            "\n",
            "Sequence 266:\n",
            "prompt: The speaker’s words resonated with the audience.\n",
            "label: unclear\n",
            "response: Who was the speaker, and what message resonated with the audience?\n",
            "\n",
            "Sequence 267:\n",
            "prompt: The restaurant was known for its creative dishes.\n",
            "label: unclear\n",
            "response: What restaurant, and what kind of dishes?\n",
            "\n",
            "Sequence 268:\n",
            "prompt: What is the capital of Australia?\n",
            "label: clear\n",
            "response: The capital of Australia is Canberra.\n",
            "\n",
            "Sequence 269:\n",
            "prompt: The animal moved quickly through the forest.\n",
            "label: unclear\n",
            "response: What animal, and why did it move quickly?\n",
            "\n",
            "Sequence 270:\n",
            "prompt: What is the speed of sound?\n",
            "label: clear\n",
            "response: The speed of sound in air is approximately 343 meters per second at 20°C.\n",
            "\n",
            "Sequence 271:\n",
            "prompt: The new software improved productivity in the office.\n",
            "label: unclear\n",
            "response: Which software, and how did it improve productivity?\n",
            "\n",
            "Sequence 272:\n",
            "prompt: The building's design was admired by many.\n",
            "label: unclear\n",
            "response: Which building, and what aspects were admired?\n",
            "\n",
            "Sequence 273:\n",
            "prompt: The beach was filled with people.\n",
            "label: unclear\n",
            "response: Which beach, and why were so many people there?\n",
            "\n",
            "Sequence 274:\n",
            "prompt: How many bones are there in the adult human body?\n",
            "label: clear\n",
            "response: There are 206 bones in the adult human body.\n",
            "\n",
            "Sequence 275:\n",
            "prompt: The conversation was getting heated.\n",
            "label: unclear\n",
            "response: What was the conversation about, and why did it get heated?\n",
            "\n",
            "Sequence 276:\n",
            "prompt: What is the main gas that makes up the Earth's atmosphere?\n",
            "label: clear\n",
            "response: The main gas in the Earth's atmosphere is nitrogen.\n",
            "\n",
            "Sequence 277:\n",
            "prompt: The book on the table is old.\n",
            "label: unclear\n",
            "response: Is the book on the table the one you’re referring to, or is it the table itself that is old?\n",
            "\n",
            "Sequence 278:\n",
            "prompt: The musician's performance captivated the audience.\n",
            "label: unclear\n",
            "response: Which musician, and what part of the performance captivated the audience?\n",
            "\n",
            "Sequence 279:\n",
            "prompt: The project was a collaborative effort.\n",
            "label: unclear\n",
            "response: Which project, and who participated in the collaboration?\n",
            "\n",
            "Sequence 280:\n",
            "prompt: What is the capital of Spain?\n",
            "label: clear\n",
            "response: The capital of Spain is Madrid.\n",
            "\n",
            "Sequence 281:\n",
            "prompt: The chef's new dish was a hit among food critics.\n",
            "label: unclear\n",
            "response: What dish, and what made it special?\n",
            "\n",
            "Sequence 282:\n",
            "prompt: What is the largest ocean on Earth?\n",
            "label: clear\n",
            "response: The largest ocean on Earth is the Pacific Ocean.\n",
            "\n",
            "Sequence 283:\n",
            "prompt: What are the three states of matter?\n",
            "label: clear\n",
            "response: The three states of matter are solid, liquid, and gas.\n",
            "\n",
            "Sequence 284:\n",
            "prompt: Who painted 'Starry Night'?\n",
            "label: clear\n",
            "response: Vincent van Gogh painted 'Starry Night.'\n",
            "\n",
            "Sequence 285:\n",
            "prompt: The painting was displayed in a popular gallery.\n",
            "label: unclear\n",
            "response: Which painting, and which gallery?\n",
            "\n",
            "Sequence 286:\n",
            "prompt: What is the main ingredient in bread?\n",
            "label: clear\n",
            "response: The main ingredient in bread is flour.\n",
            "\n",
            "Sequence 287:\n",
            "prompt: What is the capital of France?\n",
            "label: clear\n",
            "response: The capital of France is Paris.\n",
            "\n",
            "Sequence 288:\n",
            "prompt: What element does 'O' represent on the periodic table?\n",
            "label: clear\n",
            "response: The element 'O' represents oxygen.\n",
            "\n",
            "Sequence 289:\n",
            "prompt: The building was designed with innovative techniques.\n",
            "label: unclear\n",
            "response: What building, and what techniques were used?\n",
            "\n",
            "Sequence 290:\n",
            "prompt: How many planets are in our solar system?\n",
            "label: clear\n",
            "response: There are eight planets in our solar system.\n",
            "\n",
            "Sequence 291:\n",
            "prompt: She smiled at the crowd.\n",
            "label: unclear\n",
            "response: Was the smile directed at anyone specific?\n",
            "\n",
            "Sequence 292:\n",
            "prompt: What is the chemical symbol for gold?\n",
            "label: clear\n",
            "response: The chemical symbol for gold is Au.\n",
            "\n",
            "Sequence 293:\n",
            "prompt: They made a great team.\n",
            "label: unclear\n",
            "response: Who are 'they,' and what task did they accomplish together?\n",
            "\n",
            "Sequence 294:\n",
            "prompt: Who invented the telephone?\n",
            "label: clear\n",
            "response: Alexander Graham Bell invented the telephone.\n",
            "\n",
            "Sequence 295:\n",
            "prompt: Who painted the Mona Lisa?\n",
            "label: clear\n",
            "response: Leonardo da Vinci painted the Mona Lisa.\n",
            "\n",
            "Sequence 296:\n",
            "prompt: The painting is beautiful.\n",
            "label: unclear\n",
            "response: Which painting are you referring to?\n",
            "\n",
            "Sequence 297:\n",
            "prompt: Who painted the Mona Lisa?\n",
            "label: clear\n",
            "response: Leonardo da Vinci painted the Mona Lisa.\n",
            "\n",
            "Sequence 298:\n",
            "prompt: The workshop provided useful insights.\n",
            "label: unclear\n",
            "response: What workshop, and what insights were provided?\n",
            "\n",
            "Sequence 299:\n",
            "prompt: The book was filled with historical references.\n",
            "label: unclear\n",
            "response: What book, and what historical events were referenced?\n",
            "\n",
            "Sequence 300:\n",
            "prompt: The students finished their homework.\n",
            "label: unclear\n",
            "response: Which homework are you referring to?\n",
            "\n",
            "Sequence 301:\n",
            "prompt: The book was filled with unexpected twists.\n",
            "label: unclear\n",
            "response: What book, and what kind of twists were there?\n",
            "\n",
            "Sequence 302:\n",
            "prompt: The artist's latest painting has a profound message.\n",
            "label: unclear\n",
            "response: Which artist, and what message is in the painting?\n",
            "\n",
            "Sequence 303:\n",
            "prompt: What is the tallest building in the world?\n",
            "label: clear\n",
            "response: The tallest building in the world is the Burj Khalifa.\n",
            "\n",
            "Sequence 304:\n",
            "prompt: The storm brought unexpected consequences.\n",
            "label: unclear\n",
            "response: What storm, and what were the consequences?\n",
            "\n",
            "Sequence 305:\n",
            "prompt: What is the currency of the United Kingdom?\n",
            "label: clear\n",
            "response: The currency of the United Kingdom is the British pound.\n",
            "\n",
            "Sequence 306:\n",
            "prompt: The book's plot twists kept the readers on edge.\n",
            "label: unclear\n",
            "response: Which book, and what were the main plot twists?\n",
            "\n",
            "Sequence 307:\n",
            "prompt: The view from the window was amazing.\n",
            "label: unclear\n",
            "response: What did the view look like?\n",
            "\n",
            "Sequence 308:\n",
            "prompt: The boat drifted to the shore.\n",
            "label: unclear\n",
            "response: What kind of boat was it, and what caused it to drift?\n",
            "\n",
            "Sequence 309:\n",
            "prompt: What type of animal is a penguin?\n",
            "label: clear\n",
            "response: A penguin is a bird.\n",
            "\n",
            "Sequence 310:\n",
            "prompt: What is the boiling point of water?\n",
            "label: clear\n",
            "response: The boiling point of water is 100°C at standard atmospheric pressure.\n",
            "\n",
            "Sequence 311:\n",
            "prompt: What year did the American Declaration of Independence get signed?\n",
            "label: clear\n",
            "response: The American Declaration of Independence was signed in 1776.\n",
            "\n",
            "Sequence 312:\n",
            "prompt: How many bones are in the adult human body?\n",
            "label: clear\n",
            "response: There are 206 bones in the adult human body.\n",
            "\n",
            "Sequence 313:\n",
            "prompt: The garden was in full bloom.\n",
            "label: unclear\n",
            "response: Which garden, and what flowers were blooming?\n",
            "\n",
            "Sequence 314:\n",
            "prompt: What is the longest river in the world?\n",
            "label: clear\n",
            "response: The longest river in the world is the Nile River.\n",
            "\n",
            "Sequence 315:\n",
            "prompt: The cat sat on the mat.\n",
            "label: unclear\n",
            "response: What kind of mat are you referring to?\n",
            "\n",
            "Sequence 316:\n",
            "prompt: What year did the Titanic sink?\n",
            "label: clear\n",
            "response: The Titanic sank in 1912.\n",
            "\n",
            "Sequence 317:\n",
            "prompt: What is the largest planet in our solar system?\n",
            "label: clear\n",
            "response: The largest planet in our solar system is Jupiter.\n",
            "\n",
            "Sequence 318:\n",
            "prompt: What does 'DNA' stand for?\n",
            "label: clear\n",
            "response: 'DNA' stands for Deoxyribonucleic Acid.\n",
            "\n",
            "Sequence 319:\n",
            "prompt: The sunset was mesmerizing and full of color.\n",
            "label: unclear\n",
            "response: Where did this sunset happen, and what colors were present?\n",
            "\n",
            "Sequence 320:\n",
            "prompt: What year did World War II end?\n",
            "label: clear\n",
            "response: World War II ended in 1945.\n",
            "\n",
            "Sequence 321:\n",
            "prompt: How many sides does a hexagon have?\n",
            "label: clear\n",
            "response: A hexagon has six sides.\n",
            "\n",
            "Sequence 322:\n",
            "prompt: How many hearts does an octopus have?\n",
            "label: clear\n",
            "response: An octopus has three hearts.\n",
            "\n",
            "Sequence 323:\n",
            "prompt: What type of animal is a penguin?\n",
            "label: clear\n",
            "response: A penguin is a bird.\n",
            "\n",
            "Sequence 324:\n",
            "prompt: The lecture was full of complex concepts.\n",
            "label: unclear\n",
            "response: Which lecture, and what concepts were discussed?\n",
            "\n",
            "Sequence 325:\n",
            "prompt: What is the main language spoken in Brazil?\n",
            "label: clear\n",
            "response: The main language spoken in Brazil is Portuguese.\n",
            "\n",
            "Sequence 326:\n",
            "prompt: The actor won an award for his role.\n",
            "label: unclear\n",
            "response: Who is the actor, and what was the role?\n",
            "\n",
            "Sequence 327:\n",
            "prompt: The computer crashed while working on the project.\n",
            "label: unclear\n",
            "response: Which computer, and what project were you working on?\n",
            "\n",
            "Sequence 328:\n",
            "prompt: The story had a touching ending.\n",
            "label: unclear\n",
            "response: What story are you referring to, and why was it touching?\n",
            "\n",
            "Sequence 329:\n",
            "prompt: The lake was surrounded by lush vegetation.\n",
            "label: unclear\n",
            "response: Which lake, and what type of vegetation was present?\n",
            "\n",
            "Sequence 330:\n",
            "prompt: How many planets are there in our Solar System?\n",
            "label: clear\n",
            "response: There are eight planets in our Solar System.\n",
            "\n",
            "Sequence 331:\n",
            "prompt: The art piece was intriguing and thought-provoking.\n",
            "label: unclear\n",
            "response: What art piece, and why was it thought-provoking?\n",
            "\n",
            "Sequence 332:\n",
            "prompt: The event was filled with music and laughter.\n",
            "label: unclear\n",
            "response: What event, and what kind of music and laughter?\n",
            "\n",
            "Sequence 333:\n",
            "prompt: What year did World War II end?\n",
            "label: clear\n",
            "response: World War II ended in 1945.\n",
            "\n",
            "Sequence 334:\n",
            "prompt: Who invented the telephone?\n",
            "label: clear\n",
            "response: Alexander Graham Bell invented the telephone.\n",
            "\n",
            "Sequence 335:\n",
            "prompt: What is the tallest animal in the world?\n",
            "label: clear\n",
            "response: The tallest animal in the world is the giraffe.\n",
            "\n",
            "Sequence 336:\n",
            "prompt: The show featured a mix of comedy and drama.\n",
            "label: unclear\n",
            "response: What show, and how was the comedy and drama blended?\n",
            "\n",
            "Sequence 337:\n",
            "prompt: How many moons does Jupiter have?\n",
            "label: clear\n",
            "response: Jupiter has 95 known moons.\n",
            "\n",
            "Sequence 338:\n",
            "prompt: The forest was dense and full of life.\n",
            "label: unclear\n",
            "response: Which forest, and what kind of life was present?\n",
            "\n",
            "Sequence 339:\n",
            "prompt: What is the square root of 64?\n",
            "label: clear\n",
            "response: The square root of 64 is 8.\n",
            "\n",
            "Sequence 340:\n",
            "prompt: The forest was filled with mysterious sounds.\n",
            "label: unclear\n",
            "response: What forest, and what kind of sounds were heard?\n",
            "\n",
            "Sequence 341:\n",
            "prompt: Can you pass the salt?\n",
            "label: unclear\n",
            "response: Are you asking for salt to be passed to you or for something else?\n",
            "\n",
            "Sequence 342:\n",
            "prompt: What is the name of the largest desert in the world?\n",
            "label: clear\n",
            "response: The largest desert in the world is the Antarctic Desert.\n",
            "\n",
            "Sequence 343:\n",
            "prompt: What is the formula for the area of a triangle?\n",
            "label: clear\n",
            "response: The formula is 1/2 × base × height.\n",
            "\n",
            "Sequence 344:\n",
            "prompt: Who discovered penicillin?\n",
            "label: clear\n",
            "response: Alexander Fleming discovered penicillin.\n",
            "\n",
            "Sequence 345:\n",
            "prompt: What is the capital of France?\n",
            "label: clear\n",
            "response: The capital of France is Paris.\n",
            "\n",
            "Sequence 346:\n",
            "prompt: Who wrote 'Pride and Prejudice'?\n",
            "label: clear\n",
            "response: Jane Austen wrote 'Pride and Prejudice'.\n",
            "\n",
            "Sequence 347:\n",
            "prompt: The machine had a modern and sleek design.\n",
            "label: unclear\n",
            "response: What machine, and what made its design modern?\n",
            "\n",
            "Sequence 348:\n",
            "prompt: What is the tallest statue in the world?\n",
            "label: clear\n",
            "response: The tallest statue in the world is the Statue of Unity in India.\n",
            "\n",
            "Sequence 349:\n",
            "prompt: The lecture was well-received by the students.\n",
            "label: unclear\n",
            "response: Who gave the lecture, and what was it about?\n",
            "\n",
            "Sequence 350:\n",
            "prompt: The lecture included surprising facts.\n",
            "label: unclear\n",
            "response: What lecture, and what were the surprising facts?\n",
            "\n",
            "Sequence 351:\n",
            "prompt: The noise from the construction was constant.\n",
            "label: unclear\n",
            "response: What kind of construction was it, and where did it take place?\n",
            "\n",
            "Sequence 352:\n",
            "prompt: The singer’s voice was powerful and moving.\n",
            "label: unclear\n",
            "response: Who is the singer, and what made their voice moving?\n",
            "\n",
            "Sequence 353:\n",
            "prompt: The boat drifted across the lake.\n",
            "label: unclear\n",
            "response: What boat, and where was it drifting?\n",
            "\n",
            "Sequence 354:\n",
            "prompt: The concert was a highlight of the year.\n",
            "label: unclear\n",
            "response: Which concert, and what made it a highlight?\n",
            "\n",
            "Sequence 355:\n",
            "prompt: What year did the Titanic sink?\n",
            "label: clear\n",
            "response: The Titanic sank in 1912.\n",
            "\n",
            "Sequence 356:\n",
            "prompt: The workshop provided useful insights.\n",
            "label: unclear\n",
            "response: Which workshop, and what insights did it provide?\n",
            "\n",
            "Sequence 357:\n",
            "prompt: The lecture was difficult to follow.\n",
            "label: unclear\n",
            "response: Who gave the lecture, and what made it difficult to follow?\n",
            "\n",
            "Sequence 358:\n",
            "prompt: The book kept readers on the edge of their seats.\n",
            "label: unclear\n",
            "response: Which book, and what made it so engaging?\n",
            "\n",
            "Sequence 359:\n",
            "prompt: What is the currency of Japan?\n",
            "label: clear\n",
            "response: The currency of Japan is the Japanese yen.\n",
            "\n",
            "Sequence 360:\n",
            "prompt: What is the largest mammal in the world?\n",
            "label: clear\n",
            "response: The largest mammal in the world is the blue whale.\n",
            "\n",
            "Sequence 361:\n",
            "prompt: What is the freezing point of water?\n",
            "label: clear\n",
            "response: The freezing point of water is 0°C or 32°F.\n",
            "\n",
            "Sequence 362:\n",
            "prompt: The movie had several plot twists.\n",
            "label: unclear\n",
            "response: Which movie, and what were the plot twists?\n",
            "\n",
            "Sequence 363:\n",
            "prompt: Who is credited with inventing the first practical telephone?\n",
            "label: clear\n",
            "response: Alexander Graham Bell is credited with inventing the first practical telephone.\n",
            "\n",
            "Sequence 364:\n",
            "prompt: The man walked into the room and started speaking.\n",
            "label: unclear\n",
            "response: Was the speaking part related to the man walking into the room?\n",
            "\n",
            "Sequence 365:\n",
            "prompt: What does DNA stand for?\n",
            "label: clear\n",
            "response: DNA stands for Deoxyribonucleic Acid.\n",
            "\n",
            "Sequence 366:\n",
            "prompt: Who discovered penicillin?\n",
            "label: clear\n",
            "response: Alexander Fleming discovered penicillin.\n",
            "\n",
            "Sequence 367:\n",
            "prompt: The novel was both tragic and uplifting.\n",
            "label: unclear\n",
            "response: Which novel, and what elements were tragic and uplifting?\n",
            "\n",
            "Sequence 368:\n",
            "prompt: Who is the author of 'Pride and Prejudice'?\n",
            "label: clear\n",
            "response: The author of 'Pride and Prejudice' is Jane Austen.\n",
            "\n",
            "Sequence 369:\n",
            "prompt: The game had a competitive edge.\n",
            "label: unclear\n",
            "response: What game, and what made it competitive?\n",
            "\n",
            "Sequence 370:\n",
            "prompt: The book was a page-turner.\n",
            "label: unclear\n",
            "response: What book, and why was it engaging?\n",
            "\n",
            "Sequence 371:\n",
            "prompt: The movie had a surprising twist at the end.\n",
            "label: unclear\n",
            "response: What movie are you referring to, and what was the twist?\n",
            "\n",
            "Sequence 372:\n",
            "prompt: The holiday was celebrated with enthusiasm.\n",
            "label: unclear\n",
            "response: What holiday, and what made it so special?\n",
            "\n",
            "Sequence 373:\n",
            "prompt: What is the boiling point of water at sea level?\n",
            "label: clear\n",
            "response: The boiling point of water at sea level is 100°C or 212°F.\n",
            "\n",
            "Sequence 374:\n",
            "prompt: The book was a blend of mystery and romance.\n",
            "label: unclear\n",
            "response: What book, and how did it combine these genres?\n",
            "\n",
            "Sequence 375:\n",
            "prompt: How many hours are in a day?\n",
            "label: clear\n",
            "response: There are 24 hours in a day.\n",
            "\n",
            "Sequence 376:\n",
            "prompt: The speaker captivated the audience with their storytelling.\n",
            "label: unclear\n",
            "response: Which speaker, and what story was told?\n",
            "\n",
            "Sequence 377:\n",
            "prompt: The chef prepared the dish with a lot of love.\n",
            "label: unclear\n",
            "response: Does 'love' refer to affection or a special ingredient?\n",
            "\n",
            "Sequence 378:\n",
            "prompt: The party was full of laughter and music.\n",
            "label: unclear\n",
            "response: What party, and what kind of music was played?\n",
            "\n",
            "Sequence 379:\n",
            "prompt: The island was a paradise for nature lovers.\n",
            "label: unclear\n",
            "response: Which island, and what made it a paradise for nature lovers?\n",
            "\n",
            "Sequence 380:\n",
            "prompt: The scientist's findings were revolutionary.\n",
            "label: unclear\n",
            "response: Which scientist, and what were the findings about?\n",
            "\n",
            "Sequence 381:\n",
            "prompt: Who invented the telephone?\n",
            "label: clear\n",
            "response: Alexander Graham Bell invented the telephone.\n",
            "\n",
            "Sequence 382:\n",
            "prompt: What year did man first land on the moon?\n",
            "label: clear\n",
            "response: Man first landed on the moon in 1969.\n",
            "\n",
            "Sequence 383:\n",
            "prompt: The garden was full of vibrant colors.\n",
            "label: unclear\n",
            "response: What kind of garden, and what colors were present?\n",
            "\n",
            "Sequence 384:\n",
            "prompt: Who painted the 'Mona Lisa'?\n",
            "label: clear\n",
            "response: The 'Mona Lisa' was painted by Leonardo da Vinci.\n",
            "\n",
            "Sequence 385:\n",
            "prompt: The movie was set in a dystopian future.\n",
            "label: unclear\n",
            "response: What movie, and what made the future dystopian?\n",
            "\n",
            "Sequence 386:\n",
            "prompt: What is the formula for calculating speed?\n",
            "label: clear\n",
            "response: The formula for speed is distance/time.\n",
            "\n",
            "Sequence 387:\n",
            "prompt: The hotel was beautiful, but the service was disappointing.\n",
            "label: unclear\n",
            "response: Which hotel was it, and what service did you find disappointing?\n",
            "\n",
            "Sequence 388:\n",
            "prompt: The mountain trail was both challenging and rewarding.\n",
            "label: unclear\n",
            "response: Which mountain, and what made it challenging?\n",
            "\n",
            "Sequence 389:\n",
            "prompt: What is the capital of France?\n",
            "label: clear\n",
            "response: The capital of France is Paris.\n",
            "\n",
            "Sequence 390:\n",
            "prompt: How many elements are in the periodic table?\n",
            "label: clear\n",
            "response: There are currently 118 elements in the periodic table.\n",
            "\n",
            "Sequence 391:\n",
            "prompt: Who is the CEO of Tesla?\n",
            "label: clear\n",
            "response: Elon Musk is the CEO of Tesla.\n",
            "\n",
            "Sequence 392:\n",
            "prompt: The scientist's breakthrough was unexpected.\n",
            "label: unclear\n",
            "response: Which scientist, and what was the breakthrough?\n",
            "\n",
            "Sequence 393:\n",
            "prompt: The man left his keys in the car.\n",
            "label: unclear\n",
            "response: Whose keys are we talking about—his or someone else's?\n",
            "\n",
            "Sequence 394:\n",
            "prompt: Who invented the light bulb?\n",
            "label: clear\n",
            "response: Thomas Edison is credited with inventing the light bulb.\n",
            "\n",
            "Sequence 395:\n",
            "prompt: What is the boiling point of water?\n",
            "label: clear\n",
            "response: The boiling point of water is 100°C (212°F) at sea level.\n",
            "\n",
            "Sequence 396:\n",
            "prompt: What is the longest river in Asia?\n",
            "label: clear\n",
            "response: The longest river in Asia is the Yangtze River.\n",
            "\n",
            "Sequence 397:\n",
            "prompt: What is the name of the longest river in the world?\n",
            "label: clear\n",
            "response: The longest river in the world is the Nile River.\n",
            "\n",
            "Sequence 398:\n",
            "prompt: The fish in the tank moved gracefully.\n",
            "label: unclear\n",
            "response: What kind of fish, and how did they move?\n",
            "\n",
            "Sequence 399:\n",
            "prompt: What is the second planet from the Sun?\n",
            "label: clear\n",
            "response: The second planet from the Sun is Venus.\n",
            "\n",
            "Sequence 400:\n",
            "prompt: What element has the atomic number 79?\n",
            "label: clear\n",
            "response: The element with the atomic number 79 is gold.\n",
            "\n",
            "Sequence 401:\n",
            "prompt: What year did the Titanic sink?\n",
            "label: clear\n",
            "response: The Titanic sank in 1912.\n",
            "\n",
            "Sequence 402:\n",
            "prompt: The ancient ruins were found near the river.\n",
            "label: unclear\n",
            "response: What ruins, and where exactly were they found?\n",
            "\n",
            "Sequence 403:\n",
            "prompt: What year did the Titanic sink?\n",
            "label: clear\n",
            "response: The Titanic sank in 1912.\n",
            "\n",
            "Sequence 404:\n",
            "prompt: Who painted 'The Last Supper'?\n",
            "label: clear\n",
            "response: Leonardo da Vinci painted 'The Last Supper'.\n",
            "\n",
            "Sequence 405:\n",
            "prompt: What year did the first moon landing occur?\n",
            "label: clear\n",
            "response: The first moon landing occurred in 1969.\n",
            "\n",
            "Sequence 406:\n",
            "prompt: The student made impressive progress on the project.\n",
            "label: unclear\n",
            "response: Which student, and what project was being worked on?\n",
            "\n",
            "Sequence 407:\n",
            "prompt: What is the tallest mountain in the world?\n",
            "label: clear\n",
            "response: Mount Everest is the tallest mountain in the world.\n",
            "\n",
            "Sequence 408:\n",
            "prompt: How many continents are there?\n",
            "label: clear\n",
            "response: There are seven continents.\n",
            "\n",
            "Sequence 409:\n",
            "prompt: What is the tallest building in the world?\n",
            "label: clear\n",
            "response: The tallest building in the world is the Burj Khalifa.\n",
            "\n",
            "Sequence 410:\n",
            "prompt: How long does it take for the Earth to orbit the Sun?\n",
            "label: clear\n",
            "response: It takes approximately 365.25 days for the Earth to orbit the Sun.\n",
            "\n",
            "Sequence 411:\n",
            "prompt: What type of energy is stored in food?\n",
            "label: clear\n",
            "response: The type of energy stored in food is chemical energy.\n",
            "\n",
            "Sequence 412:\n",
            "prompt: He has a book and a pen.\n",
            "label: unclear\n",
            "response: Which item is he holding or using?\n",
            "\n",
            "Sequence 413:\n",
            "prompt: What is the hardest natural substance on Earth?\n",
            "label: clear\n",
            "response: The hardest natural substance on Earth is diamond.\n",
            "\n",
            "Sequence 414:\n",
            "prompt: The view from the mountain was breathtaking.\n",
            "label: unclear\n",
            "response: What mountain are you referring to?\n",
            "\n",
            "Sequence 415:\n",
            "prompt: Who is the author of 'To Kill a Mockingbird'?\n",
            "label: clear\n",
            "response: Harper Lee is the author of 'To Kill a Mockingbird'.\n",
            "\n",
            "Sequence 416:\n",
            "prompt: What is the formula for calculating the area of a circle?\n",
            "label: clear\n",
            "response: The formula for calculating the area of a circle is πr².\n",
            "\n",
            "Sequence 417:\n",
            "prompt: The chef prepared the dish with special ingredients.\n",
            "label: unclear\n",
            "response: What dish, and what were the special ingredients?\n",
            "\n",
            "Sequence 418:\n",
            "prompt: The new app made waves in the tech community.\n",
            "label: unclear\n",
            "response: What app, and why did it make waves?\n",
            "\n",
            "Sequence 419:\n",
            "prompt: The restaurant had a long line outside.\n",
            "label: unclear\n",
            "response: Which restaurant, and what made the line so long?\n",
            "\n",
            "Sequence 420:\n",
            "prompt: The mountain range stretched as far as the eye could see.\n",
            "label: unclear\n",
            "response: Which mountain range, and what made it so impressive?\n",
            "\n",
            "Sequence 421:\n",
            "prompt: They argued for hours in the living room.\n",
            "label: unclear\n",
            "response: Who are 'they,' and what were they arguing about?\n",
            "\n",
            "Sequence 422:\n",
            "prompt: The students worked on their projects late into the night.\n",
            "label: unclear\n",
            "response: Which students, and what projects were they working on?\n",
            "\n",
            "Sequence 423:\n",
            "prompt: The competition attracted participants from all over the world.\n",
            "label: unclear\n",
            "response: What competition, and who were the participants?\n",
            "\n",
            "Sequence 424:\n",
            "prompt: The movie was great.\n",
            "label: unclear\n",
            "response: Which movie did you watch?\n",
            "\n",
            "Sequence 425:\n",
            "prompt: The chef prepared the meal with care.\n",
            "label: unclear\n",
            "response: Is 'care' referring to attention or a special ingredient?\n",
            "\n",
            "Sequence 426:\n",
            "prompt: What is the formula for water?\n",
            "label: clear\n",
            "response: The formula for water is H2O.\n",
            "\n",
            "Sequence 427:\n",
            "prompt: What is the main ingredient of a Caesar salad?\n",
            "label: clear\n",
            "response: The main ingredient is romaine lettuce.\n",
            "\n",
            "Sequence 428:\n",
            "prompt: What is the most common gas in Earth's atmosphere?\n",
            "label: clear\n",
            "response: The most common gas in Earth's atmosphere is nitrogen.\n",
            "\n",
            "Sequence 429:\n",
            "prompt: What is the speed of sound?\n",
            "label: clear\n",
            "response: The speed of sound is approximately 343 meters per second in air at sea level.\n",
            "\n",
            "Sequence 430:\n",
            "prompt: How many planets are in our solar system?\n",
            "label: clear\n",
            "response: There are eight planets in our solar system.\n",
            "\n",
            "Sequence 431:\n",
            "prompt: The city was known for its historic buildings.\n",
            "label: unclear\n",
            "response: Which city, and what buildings were historic?\n",
            "\n",
            "Sequence 432:\n",
            "prompt: The painting had an unusual style.\n",
            "label: unclear\n",
            "response: Which painting, and what was unusual about its style?\n",
            "\n",
            "Sequence 433:\n",
            "prompt: When was the Declaration of Independence signed?\n",
            "label: clear\n",
            "response: The Declaration of Independence was signed on July 4, 1776.\n",
            "\n",
            "Sequence 434:\n",
            "prompt: The day was marked by unexpected events.\n",
            "label: unclear\n",
            "response: What day, and what events were unexpected?\n",
            "\n",
            "Sequence 435:\n",
            "prompt: Who is known as the 'Father of Computers'?\n",
            "label: clear\n",
            "response: Charles Babbage is known as the 'Father of Computers.'\n",
            "\n",
            "Sequence 436:\n",
            "prompt: The new technology promised exciting changes.\n",
            "label: unclear\n",
            "response: What technology, and what kind of changes were promised?\n",
            "\n",
            "Sequence 437:\n",
            "prompt: The team celebrated their victory with enthusiasm.\n",
            "label: unclear\n",
            "response: Which team, and what event did they win?\n",
            "\n",
            "Sequence 438:\n",
            "prompt: The music resonated with everyone in the room.\n",
            "label: unclear\n",
            "response: What music, and what made it resonate with people?\n",
            "\n",
            "Sequence 439:\n",
            "prompt: The conference had speakers from around the world.\n",
            "label: unclear\n",
            "response: Which conference, and who were the speakers?\n",
            "\n",
            "Sequence 440:\n",
            "prompt: The technology was ahead of its time.\n",
            "label: unclear\n",
            "response: What technology, and what features were ahead of its time?\n",
            "\n",
            "Sequence 441:\n",
            "prompt: How many planets are in the solar system?\n",
            "label: clear\n",
            "response: There are eight planets in the solar system.\n",
            "\n",
            "Sequence 442:\n",
            "prompt: The festival attracted visitors from all over.\n",
            "label: unclear\n",
            "response: What festival, and who were the visitors?\n",
            "\n",
            "Sequence 443:\n",
            "prompt: What is the capital of Canada?\n",
            "label: clear\n",
            "response: The capital of Canada is Ottawa.\n",
            "\n",
            "Sequence 444:\n",
            "prompt: What is the capital of Australia?\n",
            "label: clear\n",
            "response: The capital of Australia is Canberra.\n",
            "\n",
            "Sequence 445:\n",
            "prompt: Who is the author of 'Pride and Prejudice'?\n",
            "label: clear\n",
            "response: The author of 'Pride and Prejudice' is Jane Austen.\n",
            "\n",
            "Sequence 446:\n",
            "prompt: The team celebrated their victory in style.\n",
            "label: unclear\n",
            "response: Which team, and how did they celebrate?\n",
            "\n",
            "Sequence 447:\n",
            "prompt: The man entered the room with a confident stride.\n",
            "label: unclear\n",
            "response: Who was the man, and what was the purpose of his entrance?\n",
            "\n",
            "Sequence 448:\n",
            "prompt: The artist's work was admired by many.\n",
            "label: unclear\n",
            "response: Which artist, and what kind of work did they create?\n",
            "\n",
            "Sequence 449:\n",
            "prompt: What language do people speak in Brazil?\n",
            "label: clear\n",
            "response: The official language of Brazil is Portuguese.\n",
            "\n",
            "Sequence 450:\n",
            "prompt: The software update caused some issues.\n",
            "label: unclear\n",
            "response: What software, and what kind of issues occurred?\n",
            "\n",
            "Sequence 451:\n",
            "prompt: Who discovered penicillin?\n",
            "label: clear\n",
            "response: Penicillin was discovered by Alexander Fleming.\n",
            "\n",
            "Sequence 452:\n",
            "prompt: What is the formula for water?\n",
            "label: clear\n",
            "response: The formula for water is H2O.\n",
            "\n",
            "Sequence 453:\n",
            "prompt: Who wrote '1984'?\n",
            "label: clear\n",
            "response: George Orwell wrote '1984'.\n",
            "\n",
            "Sequence 454:\n",
            "prompt: How many planets are in our solar system?\n",
            "label: clear\n",
            "response: There are eight planets in our solar system.\n",
            "\n",
            "Sequence 455:\n",
            "prompt: The school hosted a talent show that brought everyone together.\n",
            "label: unclear\n",
            "response: Which school, and what talent was showcased?\n",
            "\n",
            "Sequence 456:\n",
            "prompt: The road trip was an adventure for everyone involved.\n",
            "label: unclear\n",
            "response: Which road trip, and what made it an adventure?\n",
            "\n",
            "Sequence 457:\n",
            "prompt: What is the atomic number of carbon?\n",
            "label: clear\n",
            "response: The atomic number of carbon is 6.\n",
            "\n",
            "Sequence 458:\n",
            "prompt: The city was known for its unique architecture.\n",
            "label: unclear\n",
            "response: Which city, and what made the architecture unique?\n",
            "\n",
            "Sequence 459:\n",
            "prompt: What do you call a group of lions?\n",
            "label: clear\n",
            "response: A group of lions is called a pride.\n",
            "\n",
            "Sequence 460:\n",
            "prompt: What is the capital of France?\n",
            "label: clear\n",
            "response: The capital of France is Paris.\n",
            "\n",
            "Sequence 461:\n",
            "prompt: What is the hardest natural substance on Earth?\n",
            "label: clear\n",
            "response: The hardest natural substance on Earth is diamond.\n",
            "\n",
            "Sequence 462:\n",
            "prompt: The mountain was shrouded in mist.\n",
            "label: unclear\n",
            "response: Which mountain, and how did the mist affect visibility?\n",
            "\n",
            "Sequence 463:\n",
            "prompt: The experiment yielded surprising results.\n",
            "label: unclear\n",
            "response: What experiment, and what were the surprising results?\n",
            "\n",
            "Sequence 464:\n",
            "prompt: Who is known as the 'Father of Computers'?\n",
            "label: clear\n",
            "response: Charles Babbage is known as the 'Father of Computers'.\n",
            "\n",
            "Sequence 465:\n",
            "prompt: The paper is on the desk.\n",
            "label: unclear\n",
            "response: Which paper are you referring to?\n",
            "\n",
            "Sequence 466:\n",
            "prompt: The hike through the forest was filled with sights to see.\n",
            "label: unclear\n",
            "response: What forest, and what sights were there?\n",
            "\n",
            "Sequence 467:\n",
            "prompt: How many sides does a pentagon have?\n",
            "label: clear\n",
            "response: A pentagon has 5 sides.\n",
            "\n",
            "Sequence 468:\n",
            "prompt: The play was performed with a lot of energy.\n",
            "label: unclear\n",
            "response: Which play, and who were the actors?\n",
            "\n",
            "Sequence 469:\n",
            "prompt: What year did World War II end?\n",
            "label: clear\n",
            "response: World War II ended in 1945.\n",
            "\n",
            "Sequence 470:\n",
            "prompt: What is the atomic number of hydrogen?\n",
            "label: clear\n",
            "response: The atomic number of hydrogen is 1.\n",
            "\n",
            "Sequence 471:\n",
            "prompt: The company's profits soared during the quarter.\n",
            "label: unclear\n",
            "response: Which company, and what factors contributed to the profits?\n",
            "\n",
            "Sequence 472:\n",
            "prompt: The class was interesting but long.\n",
            "label: unclear\n",
            "response: Which class was it, and what made it interesting?\n",
            "\n",
            "Sequence 473:\n",
            "prompt: The old man told a story that captivated everyone.\n",
            "label: unclear\n",
            "response: What story did he tell, and why was it captivating?\n",
            "\n",
            "Sequence 474:\n",
            "prompt: What is the formula for water?\n",
            "label: clear\n",
            "response: The formula for water is H2O.\n",
            "\n",
            "Sequence 475:\n",
            "prompt: How many continents are there?\n",
            "label: clear\n",
            "response: There are seven continents.\n",
            "\n",
            "Sequence 476:\n",
            "prompt: What is the boiling point of water?\n",
            "label: clear\n",
            "response: The boiling point of water is 100°C or 212°F.\n",
            "\n",
            "Sequence 477:\n",
            "prompt: What is the boiling point of water at sea level?\n",
            "label: clear\n",
            "response: The boiling point of water at sea level is 100°C or 212°F.\n",
            "\n",
            "Sequence 478:\n",
            "prompt: What is the chemical formula for water?\n",
            "label: clear\n",
            "response: The chemical formula for water is H2O.\n",
            "\n",
            "Sequence 479:\n",
            "prompt: The software update added several new features.\n",
            "label: unclear\n",
            "response: Which software, and what new features were added?\n",
            "\n",
            "Sequence 480:\n",
            "prompt: The weather changed suddenly.\n",
            "label: unclear\n",
            "response: What kind of change did the weather undergo?\n",
            "\n",
            "Sequence 481:\n",
            "prompt: The book was filled with ancient myths.\n",
            "label: unclear\n",
            "response: What book, and what myths were included?\n",
            "\n",
            "Sequence 482:\n",
            "prompt: The novel was an emotional rollercoaster.\n",
            "label: unclear\n",
            "response: What novel, and what emotions were involved?\n",
            "\n",
            "Sequence 483:\n",
            "prompt: The city was buzzing with activity.\n",
            "label: unclear\n",
            "response: Which city, and what kind of activity was taking place?\n",
            "\n",
            "Sequence 484:\n",
            "prompt: The painting was sold for a huge amount.\n",
            "label: unclear\n",
            "response: Which painting was sold, and who bought it?\n",
            "\n",
            "Sequence 485:\n",
            "prompt: The scientist made a groundbreaking discovery.\n",
            "label: unclear\n",
            "response: Who was the scientist, and what was the discovery?\n",
            "\n",
            "Sequence 486:\n",
            "prompt: What is the chemical formula for methane?\n",
            "label: clear\n",
            "response: The chemical formula for methane is CH4.\n",
            "\n",
            "Sequence 487:\n",
            "prompt: What is the most common element in the Earth's atmosphere?\n",
            "label: clear\n",
            "response: The most common element in the Earth's atmosphere is nitrogen.\n",
            "\n",
            "Sequence 488:\n",
            "prompt: The garden was full of vibrant colors.\n",
            "label: unclear\n",
            "response: Which garden, and what colors were there?\n",
            "\n",
            "Sequence 489:\n",
            "prompt: How long does it take to fly from New York to London?\n",
            "label: clear\n",
            "response: A direct flight from New York to London typically takes about 7-8 hours.\n",
            "\n",
            "Sequence 490:\n",
            "prompt: The story had a mysterious twist.\n",
            "label: unclear\n",
            "response: What story, and what was the twist?\n",
            "\n",
            "Sequence 491:\n",
            "prompt: What is the smallest prime number?\n",
            "label: clear\n",
            "response: The smallest prime number is 2.\n",
            "\n",
            "Sequence 492:\n",
            "prompt: What is the capital of Australia?\n",
            "label: clear\n",
            "response: The capital of Australia is Canberra.\n",
            "\n",
            "Sequence 493:\n",
            "prompt: The novel was adapted into a movie with great success.\n",
            "label: unclear\n",
            "response: Which novel, and what was the movie's title?\n",
            "\n",
            "Sequence 494:\n",
            "prompt: What is the tallest mountain in the world?\n",
            "label: clear\n",
            "response: The tallest mountain in the world is Mount Everest.\n",
            "\n",
            "Sequence 495:\n",
            "prompt: The garden had a variety of plants and flowers.\n",
            "label: unclear\n",
            "response: What garden, and what types of plants and flowers were present?\n",
            "\n",
            "Sequence 496:\n",
            "prompt: What is the chemical symbol for gold?\n",
            "label: clear\n",
            "response: The chemical symbol for gold is Au.\n",
            "\n",
            "Sequence 497:\n",
            "prompt: The journey was filled with unexpected encounters.\n",
            "label: unclear\n",
            "response: What journey, and what kind of encounters?\n",
            "\n",
            "Sequence 498:\n",
            "prompt: The scientist conducted an experiment to test the hypothesis.\n",
            "label: unclear\n",
            "response: Which scientist, and what hypothesis was being tested?\n",
            "\n",
            "Sequence 499:\n",
            "prompt: The team was ready for the challenge.\n",
            "label: unclear\n",
            "response: Which team, and what challenge were they preparing for?\n",
            "\n",
            "Sequence 500:\n",
            "prompt: What is the currency of the United Kingdom?\n",
            "label: clear\n",
            "response: The currency of the United Kingdom is the British pound (GBP).\n",
            "\n",
            "Sequence 501:\n",
            "prompt: The music filled the room with energy.\n",
            "label: unclear\n",
            "response: What kind of music, and what gave it energy?\n",
            "\n",
            "Sequence 502:\n",
            "prompt: The car had an impressive acceleration.\n",
            "label: unclear\n",
            "response: What car, and what made its acceleration impressive?\n",
            "\n",
            "Sequence 503:\n",
            "prompt: What is the boiling point of water?\n",
            "label: clear\n",
            "response: The boiling point of water is 100°C (212°F) at sea level.\n",
            "\n",
            "Sequence 504:\n",
            "prompt: What is the capital of Australia?\n",
            "label: clear\n",
            "response: The capital of Australia is Canberra.\n",
            "\n",
            "Sequence 505:\n",
            "prompt: What is the national bird of the United States?\n",
            "label: clear\n",
            "response: The national bird of the United States is the Bald Eagle.\n",
            "\n",
            "Sequence 506:\n",
            "prompt: What language do people speak in Brazil?\n",
            "label: clear\n",
            "response: The official language of Brazil is Portuguese.\n",
            "\n",
            "Sequence 507:\n",
            "prompt: The temperature dropped suddenly in the evening.\n",
            "label: unclear\n",
            "response: What was the temperature before it dropped, and why did it drop?\n",
            "\n",
            "Sequence 508:\n",
            "prompt: I found a wallet on the ground.\n",
            "label: unclear\n",
            "response: Whose wallet did you find, and where exactly did you find it?\n",
            "\n",
            "Sequence 509:\n",
            "prompt: How many continents are there?\n",
            "label: clear\n",
            "response: There are seven continents.\n",
            "\n",
            "Sequence 510:\n",
            "prompt: The photo captured a beautiful moment.\n",
            "label: unclear\n",
            "response: What moment, and who was in the photo?\n",
            "\n",
            "Sequence 511:\n",
            "prompt: The machine produced an impressive output.\n",
            "label: unclear\n",
            "response: What machine, and what output was produced?\n",
            "\n",
            "Sequence 512:\n",
            "prompt: The market was bustling with activity.\n",
            "label: unclear\n",
            "response: What market, and what activities were taking place?\n",
            "\n",
            "Sequence 513:\n",
            "prompt: What is the largest planet in our solar system?\n",
            "label: clear\n",
            "response: The largest planet in our solar system is Jupiter.\n",
            "\n",
            "Sequence 514:\n",
            "prompt: What is the chemical symbol for gold?\n",
            "label: clear\n",
            "response: The chemical symbol for gold is Au.\n",
            "\n",
            "Sequence 515:\n",
            "prompt: What is the national animal of Australia?\n",
            "label: clear\n",
            "response: The national animal of Australia is the kangaroo.\n",
            "\n",
            "Sequence 516:\n",
            "prompt: The mountain peak was covered in snow.\n",
            "label: unclear\n",
            "response: Which mountain was it, and what time of year?\n",
            "\n",
            "Sequence 517:\n",
            "prompt: The tour guide explained the history of the monument.\n",
            "label: unclear\n",
            "response: Which tour guide, and which monument?\n",
            "\n",
            "Sequence 518:\n",
            "prompt: Who discovered the law of gravity?\n",
            "label: clear\n",
            "response: Sir Isaac Newton discovered the law of gravity.\n",
            "\n",
            "Sequence 519:\n",
            "prompt: The new book has a captivating story.\n",
            "label: unclear\n",
            "response: What book, and what part of the story is captivating?\n",
            "\n",
            "Sequence 520:\n",
            "prompt: The road was blocked due to an accident.\n",
            "label: unclear\n",
            "response: Which road, and what kind of accident?\n",
            "\n",
            "Sequence 521:\n",
            "prompt: The chef prepared a special dish for the guests.\n",
            "label: unclear\n",
            "response: Which chef, and what dish did they prepare?\n",
            "\n",
            "Sequence 522:\n",
            "prompt: How many bones are in the adult human body?\n",
            "label: clear\n",
            "response: There are 206 bones in the adult human body.\n",
            "\n",
            "Sequence 523:\n",
            "prompt: The restaurant had a variety of dishes.\n",
            "label: unclear\n",
            "response: Which restaurant was it, and what dishes did it serve?\n",
            "\n",
            "Sequence 524:\n",
            "prompt: What is the largest planet in the Solar System?\n",
            "label: clear\n",
            "response: The largest planet in the Solar System is Jupiter.\n",
            "\n",
            "Sequence 525:\n",
            "prompt: Who painted 'The Last Supper'?\n",
            "label: clear\n",
            "response: Leonardo da Vinci painted 'The Last Supper'.\n",
            "\n",
            "Sequence 526:\n",
            "prompt: The project was completed successfully.\n",
            "label: unclear\n",
            "response: What project are you referring to?\n",
            "\n",
            "Sequence 527:\n",
            "prompt: What is the chemical symbol for iron?\n",
            "label: clear\n",
            "response: The chemical symbol for iron is Fe.\n",
            "\n",
            "Sequence 528:\n",
            "prompt: The mountain trail was challenging but rewarding.\n",
            "label: unclear\n",
            "response: Which mountain, and what made it rewarding?\n",
            "\n",
            "Sequence 529:\n",
            "prompt: What planet is known as the 'Red Planet'?\n",
            "label: clear\n",
            "response: Mars is known as the 'Red Planet.'\n",
            "\n",
            "Sequence 530:\n",
            "prompt: The new model was released last month.\n",
            "label: unclear\n",
            "response: Which model, and what improvements did it have?\n",
            "\n",
            "Sequence 531:\n",
            "prompt: What year did the Berlin Wall fall?\n",
            "label: clear\n",
            "response: The Berlin Wall fell in 1989.\n",
            "\n",
            "Sequence 532:\n",
            "prompt: What is the highest mountain in the world?\n",
            "label: clear\n",
            "response: The highest mountain in the world is Mount Everest.\n",
            "\n",
            "Sequence 533:\n",
            "prompt: What is the atomic number of gold?\n",
            "label: clear\n",
            "response: The atomic number of gold is 79.\n",
            "\n",
            "Sequence 534:\n",
            "prompt: When did the first moon landing occur?\n",
            "label: clear\n",
            "response: The first moon landing occurred on July 20, 1969.\n",
            "\n",
            "Sequence 535:\n",
            "prompt: What animal is known as the 'King of the Jungle'?\n",
            "label: clear\n",
            "response: The lion is known as the 'King of the Jungle'.\n",
            "\n",
            "Sequence 536:\n",
            "prompt: What is the formula for calculating the area of a circle?\n",
            "label: clear\n",
            "response: The formula is πr^2.\n",
            "\n",
            "Sequence 537:\n",
            "prompt: The dog barked at the man with the telescope.\n",
            "label: unclear\n",
            "response: Is the man using the telescope, or is the man holding the telescope?\n",
            "\n",
            "Sequence 538:\n",
            "prompt: The story had unexpected twists and turns.\n",
            "label: unclear\n",
            "response: What story are you referring to, and what were the twists?\n",
            "\n",
            "Sequence 539:\n",
            "prompt: The artist’s new exhibit is receiving rave reviews.\n",
            "label: unclear\n",
            "response: Who is the artist, and what is special about the exhibit?\n",
            "\n",
            "Sequence 540:\n",
            "prompt: The new technology was innovative and efficient.\n",
            "label: unclear\n",
            "response: What technology, and what makes it innovative?\n",
            "\n",
            "Sequence 541:\n",
            "prompt: What is the main ingredient in guacamole?\n",
            "label: clear\n",
            "response: The main ingredient in guacamole is avocado.\n",
            "\n",
            "Sequence 542:\n",
            "prompt: The painting was admired for its details.\n",
            "label: unclear\n",
            "response: Which painting, and what details were admired?\n",
            "\n",
            "Sequence 543:\n",
            "prompt: The conference featured cutting-edge research.\n",
            "label: unclear\n",
            "response: What research, and who presented it?\n",
            "\n",
            "Sequence 544:\n",
            "prompt: The meal was delicious.\n",
            "label: unclear\n",
            "response: What meal was it, and who prepared it?\n",
            "\n",
            "Sequence 545:\n",
            "prompt: What is the tallest building in the world?\n",
            "label: clear\n",
            "response: The tallest building in the world is the Burj Khalifa in Dubai.\n",
            "\n",
            "Sequence 546:\n",
            "prompt: The singer performed with great energy.\n",
            "label: unclear\n",
            "response: Who was the singer, and what song did they perform?\n",
            "\n",
            "Sequence 547:\n",
            "prompt: What does 'AI' stand for?\n",
            "label: clear\n",
            "response: 'AI' stands for Artificial Intelligence.\n",
            "\n",
            "Sequence 548:\n",
            "prompt: What is the square root of 144?\n",
            "label: clear\n",
            "response: The square root of 144 is 12.\n",
            "\n",
            "Sequence 549:\n",
            "prompt: The man saw the woman with binoculars.\n",
            "label: unclear\n",
            "response: Did the man use the binoculars to see the woman, or did the woman have the binoculars?\n",
            "\n",
            "Sequence 550:\n",
            "prompt: The debate about climate change continues to spark discussions.\n",
            "label: unclear\n",
            "response: Which debate, and what specific points are discussed?\n",
            "\n",
            "Sequence 551:\n",
            "prompt: The meeting ended without any clear resolution.\n",
            "label: unclear\n",
            "response: What meeting was it, and what were the main points discussed?\n",
            "\n",
            "Sequence 552:\n",
            "prompt: The scientist made a groundbreaking discovery.\n",
            "label: unclear\n",
            "response: Which scientist, and what discovery was made?\n",
            "\n",
            "Sequence 553:\n",
            "prompt: The performance was praised for its choreography.\n",
            "label: unclear\n",
            "response: Which performance, and what choreography was praised?\n",
            "\n",
            "Sequence 554:\n",
            "prompt: What year was the Declaration of Independence signed?\n",
            "label: clear\n",
            "response: The Declaration of Independence was signed in 1776.\n",
            "\n",
            "Sequence 555:\n",
            "prompt: The play was acclaimed for its powerful performances.\n",
            "label: unclear\n",
            "response: Which play, and who were the performers?\n",
            "\n",
            "Sequence 556:\n",
            "prompt: The lecture covered advanced topics in machine learning.\n",
            "label: unclear\n",
            "response: Which lecture, and what advanced topics were discussed?\n",
            "\n",
            "Sequence 557:\n",
            "prompt: How many hearts does an octopus have?\n",
            "label: clear\n",
            "response: An octopus has three hearts.\n",
            "\n",
            "Sequence 558:\n",
            "prompt: What are the primary colors in painting?\n",
            "label: clear\n",
            "response: The primary colors in painting are red, blue, and yellow.\n",
            "\n",
            "Sequence 559:\n",
            "prompt: Who invented the telephone?\n",
            "label: clear\n",
            "response: Alexander Graham Bell invented the telephone.\n",
            "\n",
            "Sequence 560:\n",
            "prompt: Who invented the telephone?\n",
            "label: clear\n",
            "response: Alexander Graham Bell invented the telephone.\n",
            "\n",
            "Sequence 561:\n",
            "prompt: The lecture was full of intriguing points.\n",
            "label: unclear\n",
            "response: Which lecture, and what were the intriguing points?\n",
            "\n",
            "Sequence 562:\n",
            "prompt: Who painted 'Starry Night'?\n",
            "label: clear\n",
            "response: Vincent van Gogh painted 'Starry Night'.\n",
            "\n",
            "Sequence 563:\n",
            "prompt: What planet is known as the 'Red Planet'?\n",
            "label: clear\n",
            "response: Mars is known as the 'Red Planet'.\n",
            "\n",
            "Sequence 564:\n",
            "prompt: What is the main source of energy for the Earth?\n",
            "label: clear\n",
            "response: The main source of energy for the Earth is the Sun.\n",
            "\n",
            "Sequence 565:\n",
            "prompt: The sun was shining brightly.\n",
            "label: unclear\n",
            "response: Was it a clear day or were there clouds?\n",
            "\n",
            "Sequence 566:\n",
            "prompt: The discovery was a turning point in scientific research.\n",
            "label: unclear\n",
            "response: What discovery, and how was it a turning point?\n",
            "\n",
            "Sequence 567:\n",
            "prompt: What is the formula for water?\n",
            "label: clear\n",
            "response: The formula for water is H2O.\n",
            "\n",
            "Sequence 568:\n",
            "prompt: How many players are there in a soccer team?\n",
            "label: clear\n",
            "response: There are 11 players in a soccer team.\n",
            "\n",
            "Sequence 569:\n",
            "prompt: What is the freezing point of water?\n",
            "label: clear\n",
            "response: The freezing point of water is 0°C (32°F).\n",
            "\n",
            "Sequence 570:\n",
            "prompt: The bank was busy when I passed by.\n",
            "label: unclear\n",
            "response: Was the bank crowded because of customers or employees?\n",
            "\n",
            "Sequence 571:\n",
            "prompt: What is the largest ocean on Earth?\n",
            "label: clear\n",
            "response: The largest ocean on Earth is the Pacific Ocean.\n",
            "\n",
            "Sequence 572:\n",
            "prompt: The discovery changed the way we understand science.\n",
            "label: unclear\n",
            "response: What discovery, and how did it change our understanding of science?\n",
            "\n",
            "Sequence 573:\n",
            "prompt: The car had a strange noise coming from the engine.\n",
            "label: unclear\n",
            "response: What car, and what type of noise was it making?\n",
            "\n",
            "Sequence 574:\n",
            "prompt: The concert was incredible.\n",
            "label: unclear\n",
            "response: Which concert was it, and what made it incredible?\n",
            "\n",
            "Sequence 575:\n",
            "prompt: What is the largest animal on Earth?\n",
            "label: clear\n",
            "response: The largest animal on Earth is the blue whale.\n",
            "\n",
            "Sequence 576:\n",
            "prompt: The scientist's theory was complex and debated.\n",
            "label: unclear\n",
            "response: Which scientist, and what theory was debated?\n",
            "\n",
            "Sequence 577:\n",
            "prompt: The novel had an unexpected ending.\n",
            "label: unclear\n",
            "response: What novel, and what made the ending unexpected?\n",
            "\n",
            "Sequence 578:\n",
            "prompt: What is the formula for the area of a circle?\n",
            "label: clear\n",
            "response: The formula for the area of a circle is πr^2.\n",
            "\n",
            "Sequence 579:\n",
            "prompt: The park was busy with people.\n",
            "label: unclear\n",
            "response: What park, and what kind of activities were happening?\n",
            "\n",
            "Sequence 580:\n",
            "prompt: Who was the first president of the United States?\n",
            "label: clear\n",
            "response: George Washington was the first president of the United States.\n",
            "\n",
            "Sequence 581:\n",
            "prompt: What element has the atomic number 79?\n",
            "label: clear\n",
            "response: The element with the atomic number 79 is gold (Au).\n",
            "\n",
            "Sequence 582:\n",
            "prompt: Who wrote '1984'?\n",
            "label: clear\n",
            "response: George Orwell wrote '1984.'\n",
            "\n",
            "Sequence 583:\n",
            "prompt: What is the currency of the United Kingdom?\n",
            "label: clear\n",
            "response: The currency of the United Kingdom is the British pound sterling.\n",
            "\n",
            "Sequence 584:\n",
            "prompt: What is the most spoken language in the world?\n",
            "label: clear\n",
            "response: The most spoken language in the world is English.\n",
            "\n",
            "Sequence 585:\n",
            "prompt: The phone rang unexpectedly.\n",
            "label: unclear\n",
            "response: Who called, and why was it unexpected?\n",
            "\n",
            "Sequence 586:\n",
            "prompt: The research team presented their findings at the conference.\n",
            "label: unclear\n",
            "response: Which research team, and what findings were presented?\n",
            "\n",
            "Sequence 587:\n",
            "prompt: The weather changed dramatically over the weekend.\n",
            "label: unclear\n",
            "response: Where did the weather change, and how did it change?\n",
            "\n",
            "Sequence 588:\n",
            "prompt: What is the capital of Italy?\n",
            "label: clear\n",
            "response: The capital of Italy is Rome.\n",
            "\n",
            "Sequence 589:\n",
            "prompt: Who discovered America?\n",
            "label: clear\n",
            "response: Christopher Columbus is credited with discovering America in 1492.\n",
            "\n",
            "Sequence 590:\n",
            "prompt: The painting was filled with contrasting colors.\n",
            "label: unclear\n",
            "response: What painting, and what were the contrasting colors?\n",
            "\n",
            "Sequence 591:\n",
            "prompt: How many continents are there on Earth?\n",
            "label: clear\n",
            "response: There are seven continents on Earth.\n",
            "\n",
            "Sequence 592:\n",
            "prompt: What is the process of photosynthesis?\n",
            "label: clear\n",
            "response: Photosynthesis is the process by which green plants make their food using sunlight, water, and carbon dioxide.\n",
            "\n",
            "Sequence 593:\n",
            "prompt: He left the restaurant feeling satisfied.\n",
            "label: unclear\n",
            "response: Was he satisfied with the food or the service?\n",
            "\n",
            "Sequence 594:\n",
            "prompt: Who painted 'Starry Night'?\n",
            "label: clear\n",
            "response: Vincent van Gogh painted 'Starry Night.'\n",
            "\n",
            "Sequence 595:\n",
            "prompt: What is the currency used in Japan?\n",
            "label: clear\n",
            "response: The currency used in Japan is the Japanese yen (JPY).\n",
            "\n",
            "Sequence 596:\n",
            "prompt: The play was performed with incredible emotion.\n",
            "label: unclear\n",
            "response: What play, and which emotions were conveyed?\n",
            "\n",
            "Sequence 597:\n",
            "prompt: The animal shelter welcomed new volunteers for the weekend.\n",
            "label: unclear\n",
            "response: Which shelter, and who were the new volunteers?\n",
            "\n",
            "Sequence 598:\n",
            "prompt: What is the largest desert in the world?\n",
            "label: clear\n",
            "response: The largest desert in the world is the Sahara.\n",
            "\n",
            "Sequence 599:\n",
            "prompt: What is the capital of France?\n",
            "label: clear\n",
            "response: The capital of France is Paris.\n",
            "\n",
            "Sequence 600:\n",
            "prompt: Who painted 'The Starry Night'?\n",
            "label: clear\n",
            "response: Vincent van Gogh painted 'The Starry Night'.\n",
            "\n",
            "Sequence 601:\n",
            "prompt: The answer was surprising to everyone.\n",
            "label: unclear\n",
            "response: What was the question, and why was the answer surprising?\n",
            "\n",
            "Sequence 602:\n",
            "prompt: The history of the ancient civilization is fascinating.\n",
            "label: unclear\n",
            "response: Which civilization, and what part of its history is fascinating?\n",
            "\n",
            "Sequence 603:\n",
            "prompt: What year did the first man land on the moon?\n",
            "label: clear\n",
            "response: The first man landed on the moon in 1969.\n",
            "\n",
            "Sequence 604:\n",
            "prompt: The restaurant is famous for its unique dishes.\n",
            "label: unclear\n",
            "response: What restaurant, and what are the unique dishes?\n",
            "\n",
            "Sequence 605:\n",
            "prompt: What year did the Berlin Wall fall?\n",
            "label: clear\n",
            "response: The Berlin Wall fell in 1989.\n",
            "\n",
            "Sequence 606:\n",
            "prompt: The artist created something extraordinary.\n",
            "label: unclear\n",
            "response: Who was the artist, and what did they create?\n",
            "\n",
            "Sequence 607:\n",
            "prompt: What is the boiling point of water at sea level?\n",
            "label: clear\n",
            "response: The boiling point of water at sea level is 100°C or 212°F.\n",
            "\n",
            "Sequence 608:\n",
            "prompt: The cat climbed the tree with ease.\n",
            "label: unclear\n",
            "response: Which cat, and how did it climb the tree?\n",
            "\n",
            "Sequence 609:\n",
            "prompt: What is the smallest prime number?\n",
            "label: clear\n",
            "response: The smallest prime number is 2.\n",
            "\n",
            "Sequence 610:\n",
            "prompt: What is the chemical formula for methane?\n",
            "label: clear\n",
            "response: The chemical formula for methane is CH4.\n",
            "\n",
            "Sequence 611:\n",
            "prompt: What is the primary ingredient in bread?\n",
            "label: clear\n",
            "response: The primary ingredient in bread is flour.\n",
            "\n",
            "Sequence 612:\n",
            "prompt: What is the formula for calculating the area of a circle?\n",
            "label: clear\n",
            "response: The formula is A = πr^2.\n",
            "\n",
            "Sequence 613:\n",
            "prompt: The professor’s lecture was captivating.\n",
            "label: unclear\n",
            "response: Who is the professor, and what made the lecture captivating?\n",
            "\n",
            "Sequence 614:\n",
            "prompt: What is the most spoken language in the world?\n",
            "label: clear\n",
            "response: The most spoken language in the world is English.\n",
            "\n",
            "Sequence 615:\n",
            "prompt: The dog ran across the field.\n",
            "label: unclear\n",
            "response: What breed of dog, and why was it running?\n",
            "\n",
            "Sequence 616:\n",
            "prompt: Who wrote 'To Kill a Mockingbird'?\n",
            "label: clear\n",
            "response: Harper Lee wrote 'To Kill a Mockingbird'.\n",
            "\n",
            "Sequence 617:\n",
            "prompt: The singer's voice was mesmerizing.\n",
            "label: unclear\n",
            "response: Who was the singer, and what made the voice mesmerizing?\n",
            "\n",
            "Sequence 618:\n",
            "prompt: The business expansion was a significant milestone.\n",
            "label: unclear\n",
            "response: Which business, and what milestone was achieved?\n",
            "\n",
            "Sequence 619:\n",
            "prompt: The novel's plot was filled with unexpected turns.\n",
            "label: unclear\n",
            "response: Which novel, and what were the unexpected turns?\n",
            "\n",
            "Sequence 620:\n",
            "prompt: I read an interesting article yesterday.\n",
            "label: unclear\n",
            "response: What was the article about?\n",
            "\n",
            "Sequence 621:\n",
            "prompt: The musician's latest album is filled with powerful lyrics.\n",
            "label: unclear\n",
            "response: Which musician, and what lyrics were powerful?\n",
            "\n",
            "Sequence 622:\n",
            "prompt: The rain started falling as we left.\n",
            "label: unclear\n",
            "response: Was the rain sudden or gradual?\n",
            "\n",
            "Sequence 623:\n",
            "prompt: Who discovered penicillin?\n",
            "label: clear\n",
            "response: Penicillin was discovered by Alexander Fleming.\n",
            "\n",
            "Sequence 624:\n",
            "prompt: Who wrote 'To Kill a Mockingbird'?\n",
            "label: clear\n",
            "response: Harper Lee wrote 'To Kill a Mockingbird.'\n",
            "\n",
            "Sequence 625:\n",
            "prompt: The kids are playing in the yard.\n",
            "label: unclear\n",
            "response: Which yard are they playing in?\n",
            "\n",
            "Sequence 626:\n",
            "prompt: Who was the first president of the United States?\n",
            "label: clear\n",
            "response: The first president of the United States was George Washington.\n",
            "\n",
            "Sequence 627:\n",
            "prompt: What is the distance between the Earth and the Moon?\n",
            "label: clear\n",
            "response: The average distance between the Earth and the Moon is about 384,400 km.\n",
            "\n",
            "Sequence 628:\n",
            "prompt: What is the capital city of Japan?\n",
            "label: clear\n",
            "response: The capital city of Japan is Tokyo.\n",
            "\n",
            "Sequence 629:\n",
            "prompt: What is the chemical symbol for water?\n",
            "label: clear\n",
            "response: The chemical symbol for water is H2O.\n",
            "\n",
            "Sequence 630:\n",
            "prompt: The lecture was full of insights and practical examples.\n",
            "label: unclear\n",
            "response: Who gave the lecture, and what were the examples?\n",
            "\n",
            "Sequence 631:\n",
            "prompt: The new policy was controversial and sparked debates.\n",
            "label: unclear\n",
            "response: What policy, and what kind of debates did it spark?\n",
            "\n",
            "Sequence 632:\n",
            "prompt: The old man walked slowly down the road.\n",
            "label: unclear\n",
            "response: Who is the old man, and why is he walking slowly?\n",
            "\n",
            "Sequence 633:\n",
            "prompt: The film captured the essence of the era.\n",
            "label: unclear\n",
            "response: What film, and what era was captured?\n",
            "\n",
            "Sequence 634:\n",
            "prompt: The teacher gave an unexpected assignment.\n",
            "label: unclear\n",
            "response: What was the assignment about?\n",
            "\n",
            "Sequence 635:\n",
            "prompt: The city's streets were alive with activity.\n",
            "label: unclear\n",
            "response: Which city, and what kind of activity was taking place?\n",
            "\n",
            "Sequence 636:\n",
            "prompt: What is the main ingredient in chocolate?\n",
            "label: clear\n",
            "response: The main ingredient in chocolate is cocoa.\n",
            "\n",
            "Sequence 637:\n",
            "prompt: I talked to John and Sarah.\n",
            "label: unclear\n",
            "response: What was the topic of the conversation, and who was more involved?\n",
            "\n",
            "Sequence 638:\n",
            "prompt: What is the largest ocean on Earth?\n",
            "label: clear\n",
            "response: The largest ocean on Earth is the Pacific Ocean.\n",
            "\n",
            "Sequence 639:\n",
            "prompt: What is the capital of Canada?\n",
            "label: clear\n",
            "response: The capital of Canada is Ottawa.\n",
            "\n",
            "Sequence 640:\n",
            "prompt: How many states are in the United States?\n",
            "label: clear\n",
            "response: There are 50 states in the United States.\n",
            "\n",
            "Sequence 641:\n",
            "prompt: The documentary was eye-opening and informative.\n",
            "label: unclear\n",
            "response: What documentary, and what made it eye-opening?\n",
            "\n",
            "Sequence 642:\n",
            "prompt: What is the main gas in the Earth's atmosphere?\n",
            "label: clear\n",
            "response: The main gas in the Earth's atmosphere is nitrogen.\n",
            "\n",
            "Sequence 643:\n",
            "prompt: The new software update has several useful features.\n",
            "label: unclear\n",
            "response: What software, and what features were added?\n",
            "\n",
            "Sequence 644:\n",
            "prompt: Who is the author of 'The Great Gatsby'?\n",
            "label: clear\n",
            "response: The author of 'The Great Gatsby' is F. Scott Fitzgerald.\n",
            "\n",
            "Sequence 645:\n",
            "prompt: What are the three states of matter?\n",
            "label: clear\n",
            "response: The three states of matter are solid, liquid, and gas.\n",
            "\n",
            "Sequence 646:\n",
            "prompt: The animal was known for its distinctive call.\n",
            "label: unclear\n",
            "response: What animal, and what was distinctive about its call?\n",
            "\n",
            "Sequence 647:\n",
            "prompt: The book was full of unexpected twists.\n",
            "label: unclear\n",
            "response: Which book, and what were the twists?\n",
            "\n",
            "Sequence 648:\n",
            "prompt: What is the name of the world's largest desert?\n",
            "label: clear\n",
            "response: The largest desert in the world is the Sahara Desert.\n",
            "\n",
            "Sequence 649:\n",
            "prompt: Who painted the Mona Lisa?\n",
            "label: clear\n",
            "response: Leonardo da Vinci painted the Mona Lisa.\n",
            "\n",
            "Sequence 650:\n",
            "prompt: The actor's performance was intense and unforgettable.\n",
            "label: unclear\n",
            "response: What actor, and what made the performance intense?\n",
            "\n",
            "Sequence 651:\n",
            "prompt: The recipe for the dish was shared by the chef online.\n",
            "label: unclear\n",
            "response: Which recipe, and who was the chef?\n",
            "\n",
            "Sequence 652:\n",
            "prompt: The musician played a familiar tune.\n",
            "label: unclear\n",
            "response: Who was the musician, and what tune did they play?\n",
            "\n",
            "Sequence 653:\n",
            "prompt: They finished the project in record time.\n",
            "label: unclear\n",
            "response: Which project are you referring to?\n",
            "\n",
            "Sequence 654:\n",
            "prompt: The invention was ahead of its time.\n",
            "label: unclear\n",
            "response: What was the invention, and what made it ahead of its time?\n",
            "\n",
            "Sequence 655:\n",
            "prompt: The garden was in full bloom, attracting bees and butterflies.\n",
            "label: unclear\n",
            "response: Which garden, and what flowers were in bloom?\n",
            "\n",
            "Sequence 656:\n",
            "prompt: What is the symbol for gold on the periodic table?\n",
            "label: clear\n",
            "response: The symbol for gold is Au.\n",
            "\n",
            "Sequence 657:\n",
            "prompt: The report outlined key findings.\n",
            "label: unclear\n",
            "response: Which report, and what findings were included?\n",
            "\n",
            "Sequence 658:\n",
            "prompt: The company launched a new product.\n",
            "label: unclear\n",
            "response: Which company, and what product was launched?\n",
            "\n",
            "Sequence 659:\n",
            "prompt: What is the formula for calculating the area of a circle?\n",
            "label: clear\n",
            "response: The formula for the area of a circle is πr².\n",
            "\n",
            "Sequence 660:\n",
            "prompt: The detective solved the case after many challenges.\n",
            "label: unclear\n",
            "response: Which case, and what challenges did the detective face?\n",
            "\n",
            "Sequence 661:\n",
            "prompt: What year was the Declaration of Independence signed?\n",
            "label: clear\n",
            "response: The Declaration of Independence was signed in 1776.\n",
            "\n",
            "Sequence 662:\n",
            "prompt: Can you tell me about the cat?\n",
            "label: unclear\n",
            "response: Which cat do you mean? Is it a pet, a stray, or something else?\n",
            "\n",
            "Sequence 663:\n",
            "prompt: What does the acronym 'NASA' stand for?\n",
            "label: clear\n",
            "response: NASA stands for the National Aeronautics and Space Administration.\n",
            "\n",
            "Sequence 664:\n",
            "prompt: What year did World War II end?\n",
            "label: clear\n",
            "response: World War II ended in 1945.\n",
            "\n",
            "Sequence 665:\n",
            "prompt: What is the main ingredient in guacamole?\n",
            "label: clear\n",
            "response: The main ingredient in guacamole is avocado.\n",
            "\n",
            "Sequence 666:\n",
            "prompt: The garden was lush and full of life.\n",
            "label: unclear\n",
            "response: What garden, and what plants were there?\n",
            "\n",
            "Sequence 667:\n",
            "prompt: What is the speed of light?\n",
            "label: clear\n",
            "response: The speed of light in a vacuum is approximately 299,792 kilometers per second.\n",
            "\n",
            "Sequence 668:\n",
            "prompt: What is the speed of light?\n",
            "label: clear\n",
            "response: The speed of light in a vacuum is approximately 299,792 kilometers per second.\n",
            "\n",
            "Sequence 669:\n",
            "prompt: What is the primary language spoken in Brazil?\n",
            "label: clear\n",
            "response: The primary language spoken in Brazil is Portuguese.\n",
            "\n",
            "Sequence 670:\n",
            "prompt: What is the most common element in the universe?\n",
            "label: clear\n",
            "response: The most common element in the universe is hydrogen.\n",
            "\n",
            "Sequence 671:\n",
            "prompt: The chef prepared a dish that was both spicy and sweet.\n",
            "label: unclear\n",
            "response: What dish, and what ingredients made it spicy and sweet?\n",
            "\n",
            "Sequence 672:\n",
            "prompt: The path led to a hidden clearing.\n",
            "label: unclear\n",
            "response: What clearing are you talking about?\n",
            "\n",
            "Sequence 673:\n",
            "prompt: What is the process by which plants make their food?\n",
            "label: clear\n",
            "response: The process by which plants make their food is called photosynthesis.\n",
            "\n",
            "Sequence 674:\n",
            "prompt: The concert was sold out quickly.\n",
            "label: unclear\n",
            "response: Which concert are you referring to?\n",
            "\n",
            "Sequence 675:\n",
            "prompt: The storm swept through the village.\n",
            "label: unclear\n",
            "response: What kind of storm was it, and how did it affect the village?\n",
            "\n",
            "Sequence 676:\n",
            "prompt: The concert was unforgettable.\n",
            "label: unclear\n",
            "response: Which concert, and what made it unforgettable?\n",
            "\n",
            "Sequence 677:\n",
            "prompt: What is the chemical symbol for gold?\n",
            "label: clear\n",
            "response: The chemical symbol for gold is Au.\n",
            "\n",
            "Sequence 678:\n",
            "prompt: The conference was attended by renowned experts.\n",
            "label: unclear\n",
            "response: What conference, and who were the experts?\n",
            "\n",
            "Sequence 679:\n",
            "prompt: The ball is in the court.\n",
            "label: unclear\n",
            "response: Are you referring to a literal ball in a sports court, or is it a metaphor?\n",
            "\n",
            "Sequence 680:\n",
            "prompt: What is the formula for table salt?\n",
            "label: clear\n",
            "response: The formula for table salt is NaCl (sodium chloride).\n",
            "\n",
            "Sequence 681:\n",
            "prompt: How many continents are there on Earth?\n",
            "label: clear\n",
            "response: There are seven continents on Earth.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set the pad token to the EOS token\n",
        "\n",
        "# Read the content from the formatted dataset file\n",
        "with open(output_file, 'r', encoding='utf-8') as f:\n",
        "    data = f.read().split(\"\\n\\n\")  # Split the content into separate records\n",
        "\n",
        "# Tokenize each sequence\n",
        "def tokenize_function(text):\n",
        "    return tokenizer(\n",
        "        text,\n",
        "        max_length=600,  # Adjust as needed\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Tokenize all entries and collect input_ids and attention masks\n",
        "tokenized_texts = [tokenize_function(entry) for entry in data]\n",
        "\n",
        "# Create a dictionary of input IDs and attention masks\n",
        "input_ids = torch.cat([t[\"input_ids\"] for t in tokenized_texts], dim=0)\n",
        "attention_masks = torch.cat([t[\"attention_mask\"] for t in tokenized_texts], dim=0)\n",
        "\n",
        "# Create a Hugging Face dataset from the tokenized data\n",
        "dataset = Dataset.from_dict({\n",
        "    'input_ids': input_ids.tolist(),\n",
        "    'attention_mask': attention_masks.tolist()\n",
        "})"
      ],
      "metadata": {
        "id": "qqtboDBiUxvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):  # Display 5 examples (adjust as needed)\n",
        "    input_ids = dataset[i]['input_ids']\n",
        "    text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "    print(f\"Example {i + 1}:\\n{text}\\n\" + \"-\" * 50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJxyzprmfOEW",
        "outputId": "695656ee-6ff5-4164-fbd0-596a8af70b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1:\n",
            "prompt: Who is the author of '1984'?\n",
            "label: clear\n",
            "response: George Orwell is the author of '1984.'\n",
            "--------------------------------------------------\n",
            "\n",
            "Example 2:\n",
            "prompt: What is the largest planet in our Solar System?\n",
            "label: clear\n",
            "response: The largest planet in our Solar System is Jupiter.\n",
            "--------------------------------------------------\n",
            "\n",
            "Example 3:\n",
            "prompt: How many days are in a leap year?\n",
            "label: clear\n",
            "response: There are 366 days in a leap year.\n",
            "--------------------------------------------------\n",
            "\n",
            "Example 4:\n",
            "prompt: The conversation was full of interesting points.\n",
            "label: unclear\n",
            "response: What conversation, and what points were interesting?\n",
            "--------------------------------------------------\n",
            "\n",
            "Example 5:\n",
            "prompt: What does 'HTML' stand for?\n",
            "label: clear\n",
            "response: HTML stands for HyperText Markup Language.\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training model"
      ],
      "metadata": {
        "id": "4dRJwDSbWWsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,  # Batch size per device\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=5,\n",
        "    warmup_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=100,\n",
        "    output_dir='outputs',\n",
        "    fp16=True  # Use mixed precision\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=loaded_model,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Disable cache for training\n",
        "loaded_model.config.use_cache = False\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(loaded_model.state_dict(), 'gpt_neo_finetuned.pt')\n",
        "\n",
        "# Verify the device of the model\n",
        "print(f\"Model is on device: {next(loaded_model.parameters()).device}\")\n",
        "\n",
        "# Extract the loss values from the trainer's logs\n",
        "train_loss = []\n",
        "for log in trainer.state.log_history:\n",
        "    if 'loss' in log:\n",
        "        train_loss.append(log['loss'])\n",
        "\n",
        "# Plot the loss after training\n",
        "plt.plot(train_loss)\n",
        "plt.title('Training Loss over Time')\n",
        "plt.xlabel('Steps (Every 100 steps)')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "lnr69hy3PnQl",
        "outputId": "f2f1adaa-db91-4917-c54f-f4cdf509e7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='302' max='425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [302/425 14:09 < 05:48, 0.35 it/s, Epoch 3.53/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.888900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.498700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.429400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-5832e316a058>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Save the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2123\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2124\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2484\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m                     ):\n\u001b[1;32m   2488\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "loaded_model.save_pretrained(os.path.join('/content/drive/MyDrive/cmm', \"final_modelsss3\"))\n",
        "tokenizer.save_pretrained(os.path.join('/content/drive/MyDrive/cmm', \"final_modelsss3\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve-B_m0VlG8c",
        "outputId": "9c6f8118-ae12-4bfb-b2ee-a2807bd3b4a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/cmm/final_modelsss3/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/cmm/final_modelsss3/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/cmm/final_modelsss3/vocab.json',\n",
              " '/content/drive/MyDrive/cmm/final_modelsss3/merges.txt',\n",
              " '/content/drive/MyDrive/cmm/final_modelsss3/added_tokens.json',\n",
              " '/content/drive/MyDrive/cmm/final_modelsss3/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing model:"
      ],
      "metadata": {
        "id": "M25sSoEOcb6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive\n",
        "!rm -rf /content/drive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mXkFCAmmRDx",
        "outputId": "09066032-86b6-4c3e-a4cd-032fa6ade428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYlQTvvflZpM",
        "outputId": "5587611e-254a-40e8-9847-deeb79c03141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example prompt for testing\n",
        "prompt = \"prompt: How many days are in a leap year?\\nlabel: \"\n",
        "# Tokenize the prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate the output\n",
        "loaded_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = loaded_model.generate(\n",
        "        **inputs,\n",
        "        max_length=100,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "# Decode the output\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Stop processing at the second occurrence of \"\\nprompt:\"\n",
        "split_response = response.split(\"\\nprompt:\", 2)\n",
        "if len(split_response) > 1:\n",
        "    final_response = split_response[0] + \"\\nprompt:\" + split_response[1]\n",
        "else:\n",
        "    final_response = response\n",
        "\n",
        "# Print the prompt and the model's response\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"\\nModel Response:\")\n",
        "print(final_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNh9-xzGcfUr",
        "outputId": "93032ce5-5492-44b1-d304-035b973ca019"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "prompt: How many days are in a leap year?\n",
            "label: \n",
            "\n",
            "Model Response:\n",
            "prompt: How many days are in a leap year?\n",
            "label:  clear\n",
            "response: There are 366 days in a leap year.\n",
            "response: What is a leap year?\n",
            "response: A leap year is a year that has 366 days.\n",
            "response: What is a leap day?\n",
            "Response: A leap day is a day that has 366 hours and 21 minutes.\n",
            "Response: What is a leap hour?\n",
            "Response: A leap hour is a hour that has 366 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "UgMMDuiUgiV6",
        "outputId": "75e284da-b287-4150-cb76-7bdbcb4fe697"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"label: clear\\nprompt: What is the capital of France?\\nresponse: The capital of France is Paris.\\nclear: What is the capital of France?\\nresponse: The capital of France is Paris.\\nclear: Which year was France founded?\\nResponse: France was founded in 1791.\\nclear: Which year was France founded?\\nResponse: 1791 was the year of France's founding.\\nclear: Which year was France founded?\\nResponse: 1791 was\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-refinement"
      ],
      "metadata": {
        "id": "9VUPa7_WYBFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the GPT-Neo 1.3B model and tokenizer\n",
        "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = loaded_model\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Load a pre-trained NLI model for evaluating response coherence\n",
        "nli_model_name = \"facebook/bart-large-mnli\"\n",
        "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name)\n",
        "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
        "nli_model.eval()\n",
        "\n",
        "# Function for generating initial response\n",
        "def generate_initial_response(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=150,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    initial_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return initial_response\n",
        "\n",
        "# Function for generating a self-review\n",
        "def generate_self_review(response):\n",
        "    review_prompt = f\"Review the following response and suggest improvements: {response}\"\n",
        "    review_inputs = tokenizer(review_prompt, return_tensors=\"pt\").to(device)\n",
        "    review_outputs = model.generate(\n",
        "        **review_inputs,\n",
        "        max_length=200,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    review = tokenizer.decode(review_outputs[0], skip_special_tokens=True)\n",
        "    return review\n",
        "\n",
        "# Function for evaluating with NLI\n",
        "def evaluate_with_nli(prompt, response):\n",
        "    nli_input = nli_tokenizer.encode(prompt, response, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        nli_outputs = nli_model(nli_input)\n",
        "        nli_logits = nli_outputs.logits\n",
        "        nli_predictions = torch.argmax(nli_logits, dim=-1)\n",
        "    return nli_predictions.item()\n",
        "\n",
        "# Function to refine the response based on self-review and NLI\n",
        "def refine_response(prompt, initial_response, review):\n",
        "    refined_prompt = f\"Improve the following response based on the feedback: {initial_response} Feedback: {review}\"\n",
        "    refined_inputs = tokenizer(refined_prompt, return_tensors=\"pt\").to(device)\n",
        "    refined_outputs = model.generate(\n",
        "        **refined_inputs,\n",
        "        max_length=600,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    refined_response = tokenizer.decode(refined_outputs[0], skip_special_tokens=True)\n",
        "    return refined_response\n",
        "\n",
        "# Main function to perform self-refinement with NLI\n",
        "def generate_response_with_self_refinement(prompt):\n",
        "    initial_response = generate_initial_response(prompt)\n",
        "    print(f\"Initial Response:\\n{initial_response}\\n\")\n",
        "\n",
        "    # Generate a self-review\n",
        "    self_review = generate_self_review(initial_response)\n",
        "    print(f\"Self-Review:\\n{self_review}\\n\")\n",
        "\n",
        "    # Evaluate initial response with NLI\n",
        "    nli_score = evaluate_with_nli(prompt, initial_response)\n",
        "    print(f\"NLI Score: {nli_score}\\n\")\n",
        "\n",
        "    # Append an NLI message to the feedback if the score is below the threshold\n",
        "    if nli_score < 1:  # Adjust the threshold as needed\n",
        "        additional_feedback = \"The response is logically not correct. Consider revising the content for coherence.\"\n",
        "        self_review += f\" {additional_feedback}\"\n",
        "\n",
        "    # Print the updated self-review\n",
        "    print(f\"Updated Self-Review:\\n{self_review}\\n\")\n",
        "\n",
        "    # Refine the response based on self-review\n",
        "    refined_response = refine_response(prompt, initial_response, self_review)\n",
        "    print(f\"Refined Response:\\n{refined_response}\")\n",
        "\n",
        "# Example usage\n",
        "prompt = \"The novel was adapted into a movie with great success.\"\n",
        "generate_response_with_self_refinement(prompt)\n"
      ],
      "metadata": {
        "id": "PumzgVUnQFV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bf890a-b279-4fd5-ebcc-9664db3c1094"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Response:\n",
            "The novel was adapted into a movie with great success. Why is it so successful?\n",
            "\n",
            "The novel was adapted into a movie with great success. Why is it so successful?\n",
            "\n",
            "Because it was both a character study and a story about human relationships.\n",
            "\n",
            "How do you think that movie made you want to read it again?\n",
            "\n",
            "The movie made me want to read it again because I was enjoying the character study and the story itself.\n",
            "\n",
            "What kind of character study was it?\n",
            "\n",
            "It was about a man who was a doctor, but he was also a writer.\n",
            "\n",
            "What made the doctor a character in the movie?\n",
            "\n",
            "One of the things that made the doctor a character in the movie was that he\n",
            "\n",
            "Self-Review:\n",
            "Review the following response and suggest improvements: The novel was adapted into a movie with great success. Why is it so successful?\n",
            "\n",
            "The novel was adapted into a movie with great success. Why is it so successful?\n",
            "\n",
            "Because it was both a character study and a story about human relationships.\n",
            "\n",
            "How do you think that movie made you want to read it again?\n",
            "\n",
            "The movie made me want to read it again because I was enjoying the character study and the story itself.\n",
            "\n",
            "What kind of character study was it?\n",
            "\n",
            "It was about a man who was a doctor, but he was also a writer.\n",
            "\n",
            "What made the doctor a character in the movie?\n",
            "\n",
            "One of the things that made the doctor a character in the movie was that he had a unique way of communicating with his patients.\n",
            "\n",
            "What kind of story was it about?\n",
            "\n",
            "It was a science fiction story about a man who was a doctor, but he was also a writer\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLI Score: 1\n",
            "\n",
            "Updated Self-Review:\n",
            "Review the following response and suggest improvements: The novel was adapted into a movie with great success. Why is it so successful?\n",
            "\n",
            "The novel was adapted into a movie with great success. Why is it so successful?\n",
            "\n",
            "Because it was both a character study and a story about human relationships.\n",
            "\n",
            "How do you think that movie made you want to read it again?\n",
            "\n",
            "The movie made me want to read it again because I was enjoying the character study and the story itself.\n",
            "\n",
            "What kind of character study was it?\n",
            "\n",
            "It was about a man who was a doctor, but he was also a writer.\n",
            "\n",
            "What made the doctor a character in the movie?\n",
            "\n",
            "One of the things that made the doctor a character in the movie was that he had a unique way of communicating with his patients.\n",
            "\n",
            "What kind of story was it about?\n",
            "\n",
            "It was a science fiction story about a man who was a doctor, but he was also a writer\n",
            "\n",
            "Refined Response:\n",
            "Improve the following response based on the feedback: The novel was adapted into a movie with great success. Why is it so successful?\n",
            "\n",
            "The novel was adapted into a movie with great success. Why is it so successful?\n",
            "\n",
            "Because it was both a character study and a story about human relationships.\n",
            "\n",
            "How do you think that movie made you want to read it again?\n",
            "\n",
            "The movie made me want to read it again because I was enjoying the character study and the story itself.\n",
            "\n",
            "What kind of character study was it?\n",
            "\n",
            "It was about a man who was a doctor, but he was also a writer.\n",
            "\n",
            "What made the doctor a character in the movie?\n",
            "\n",
            "One of the things that made the doctor a character in the movie was that he Feedback: Review the following response and suggest improvements: The novel was adapted into a movie with great success. Why is it so successful?\n",
            "\n",
            "The novel was adapted into a movie with great success. Why is it so successful?\n",
            "\n",
            "Because it was both a character study and a story about human relationships.\n",
            "\n",
            "How do you think that movie made you want to read it again?\n",
            "\n",
            "The movie made me want to read it again because I was enjoying the character study and the story itself.\n",
            "\n",
            "What kind of character study was it?\n",
            "\n",
            "It was about a man who was a doctor, but he was also a writer.\n",
            "\n",
            "What made the doctor a character in the movie?\n",
            "\n",
            "One of the things that made the doctor a character in the movie was that he had a unique way of communicating with his patients.\n",
            "\n",
            "What kind of story was it about?\n",
            "\n",
            "It was a science fiction story about a man who was a doctor, but he was also a writer.\n",
            "\n",
            "What kind of relationship did the doctor have with his patients?\n",
            "\n",
            "He was a doctor, but he also had a unique way of communicating with his patients.\n",
            "\n",
            "How did he interact with his patients in the novel?\n",
            "\n",
            "He interacted with his patients in the novel by communicating with his patients through the voice of his wife.\n",
            "\n",
            "Does the novel have an ending?\n",
            "\n",
            "The novel has an ending, but it was a different ending than the movie.\n",
            "\n",
            "What was the ending?\n",
            "\n",
            "The ending was not the same as the movie.\n",
            "\n",
            "What was the ending about?\n",
            "\n",
            "The ending was about the relationship between the doctor and his wife.\n",
            "\n",
            "What was the doctor's relationship with his wife like?\n",
            "\n",
            "She was a scientist, but she also had a unique way of communicating with her patients.\n",
            "\n",
            "How did the doctor interact with his patients in the novel?\n",
            "\n",
            "The doctor interacted with his patients in the novel by communicating with his wife through the voice of his computer.\n",
            "\n",
            "What was the doctor's relationship with his wife like when he interacted with his patients in the novel?\n",
            "\n",
            "She was a scientist, but\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xGpqU9b4vBuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}